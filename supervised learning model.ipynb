{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to supervised learning part firstly we need to perform the same data preprocessing and cleaning as earlier\n",
    "def cleaning_data(train_data):\n",
    "    columns_to_drop = ['ALTER_KIND1', 'ALTER_KIND2', 'ALTER_KIND3', 'ALTER_KIND4', 'EXTSEL992', 'KK_KUNDENTYP']\n",
    "    train_data.drop(columns = columns_to_drop, inplace=True)\n",
    "    k = 0\n",
    "    for i,j in var_unknown.items():\n",
    "        if i in columns_to_remove:\n",
    "            continue\n",
    "        if i in columns_to_rename:\n",
    "            train_data[columns_renamed[k]] = train_data[columns_renamed[k]].replace(j, np.nan)\n",
    "            continue\n",
    "        train_data[i] = train_data[i].replace(j, np.nan)\n",
    "    #'RELAT_AB' need to replace 9 with nan\n",
    "    train_data['RELAT_AB'] = train_data['RELAT_AB'].replace([9], np.nan)\n",
    "    columns_to_drop_1 = ['AGER_TYP', 'KBA05_BAUMAX', 'TITEL_KZ']\n",
    "    train_data.drop(columns = columns_to_drop_1, inplace=True)\n",
    "\n",
    "    #dropping 'LP_STATUS_GROB', 'LP_FAMILIE_GROB' because similar to 'LP_STATUS_FEIN' and 'LP_FAMILIE_FEIN' \n",
    "    train_data.drop(columns = ['LP_STATUS_GROB', 'LP_FAMILIE_GROB'], inplace=True)\n",
    "    #dropping LNR\n",
    "    train_data.drop(columns = ['LNR'], inplace=True)\n",
    "\n",
    "    #reengineering features\n",
    "    #PLZ8_BAUMAX, how many family homes\n",
    "    train_data['PLZ8_BAUMAX'] = train_data['PLZ8_BAUMAX'].replace([5], 0)\n",
    "    #if person lives in business building or not\n",
    "    train_data['PLZ8_BAUMAX_BIZ'] = train_data['PLZ8_BAUMAX'].replace([0,1,2,3,4], [1,0,0,0,0])\n",
    "    #main of avant-garde\n",
    "    train_data['PRAEGENDE_MAIN_OR_AVANT'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [0,1,0,1,0,1,1,0,1,0,1,0,1,0,1])\n",
    "    #40ies,50ies,60ies,70ies,80ies,90ies movements\n",
    "    train_data['PRAEGENDE_JUGENDJAHRE'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [1,1,2,2,3,3,3,4,4,5,5,5,5,6,6])\n",
    "\n",
    "    #CAMEO_INTL\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].astype(float)\n",
    "    train_data['CAMEO_DEUG_2015'] = train_data['CAMEO_DEUG_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_DEUG_2015']  = train_data['CAMEO_DEUG_2015'].astype(float)\n",
    "    #household income\n",
    "    train_data['CAMEO_HOUSEHOLD'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                  41, 42, 43, 44, 45, 51, 52, 53, 54, 55],[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5])\n",
    "    #family type \n",
    "    train_data['CAMEO_FAMILY_TYPE'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                   41, 42, 43, 44, 45, 51, 52, 53, 54, 55], [1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5])\n",
    "\n",
    "    #OST_WEST_KZ encode 'W' to 0 and '0' to 1\n",
    "    train_data['OST_WEST_KZ'] = train_data['OST_WEST_KZ'].replace(['W', 'O'], [0,1]) \n",
    "    #one hot encoding of 'ANREDE_KZ'\n",
    "    train_data = pd.get_dummies(train_data, columns=['ANREDE_KZ'], drop_first=True)\n",
    "\n",
    "\n",
    "    train_data.drop(columns = ['CAMEO_INTL_2015'], inplace=True)\n",
    "    #for WOHNLAGE, replace 0 with nan\n",
    "    train_data['WOHNLAGE'] = train_data['WOHNLAGE'].replace([0], np.nan)\n",
    "    #create new WOHNLAGE feature, urban or rural\n",
    "    train_data['WOHNLAGE_REGION'] = train_data['WOHNLAGE'].replace([1,2,3,4,5,7,8],[0,0,0,0,0,1,1])\n",
    "\n",
    "    #dropping CAMEO_DEU, too many categories \n",
    "    train_data.drop(columns = ['CAMEO_DEU_2015'], inplace=True)\n",
    "    #dropping LP_LEBENSPHASE_FEIN, too many categories\n",
    "    train_data.drop(columns = ['LP_LEBENSPHASE_FEIN'], inplace=True)\n",
    "    #dropping D19_LETZTER_KAUF_BRANCHE, duplicated information\n",
    "    train_data.drop(columns=['D19_LETZTER_KAUF_BRANCHE'], inplace=True)\n",
    "\n",
    "    #converting 'EINGEFUEGT_AM' to total months\n",
    "    eingefuegt = np.empty(shape=(train_data.shape[0],1))\n",
    "    k = 0\n",
    "    train_data['EINGEFUEGT_AM'] = train_data['EINGEFUEGT_AM'].fillna(-1)\n",
    "    for i in train_data['EINGEFUEGT_AM']:\n",
    "        if i != -1:\n",
    "\n",
    "            yrmthday, zeros = i.split()\n",
    "            mth_1 = (2019 - (int(yrmthday[0:4]) + 1)) * 12\n",
    "            mth_2 = 12 - int(yrmthday[5:6])\n",
    "            mth_3 = (30 - int(yrmthday[8:10])) / 30\n",
    "            mth_total = mth_1 + mth_2 + mth_3\n",
    "            eingefuegt[k] = mth_total\n",
    "        else:\n",
    "            eingefuegt[k] = -1\n",
    "        k += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data.drop(columns =['EINGEFUEGT_AM'] , inplace=True)\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = pd.DataFrame(eingefuegt)\n",
    "\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = train_data['EINGEFUEGT_MODIFIED'].replace([-1],train_data['EINGEFUEGT_MODIFIED'].mode().values)\n",
    "    \n",
    "    return train_data\n",
    "    \n",
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';', index_col=0)\n",
    "mailout_train = cleaning_data(mailout_train)\n",
    "mailout_train['TARGET'] = mailout_train['RESPONSE']\n",
    "mailout_train.drop(columns=['RESPONSE'], inplace=True)\n",
    "\n",
    "#I would need to read in MAILOUT TEST and perform the same cleaning and preprocessing steps as before\n",
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';', index_col=0)\n",
    "mailout_test = cleaning_data(mailout_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting MAILOUT TRAIN, there is only 532 positive responses, hence this is an imbalanced dataset\n",
    "print(mailout_train.shape[0])\n",
    "print(np.count_nonzero(mailout_train['TARGET'] == 1))\n",
    "\n",
    "#firstly we will train a model using MAILOUT TRAIN as the training data and 'TARGET' variable as the target variable\n",
    "#I will only mention XGBoost here since it gives me the highest AUC score in Kaggle competition\n",
    "\n",
    "# I will be using bayesian optimisation to find the hyperparameter combination that give the best cross-validation mean score which is then uploaded to Kaggle.\n",
    "import ast\n",
    "import csv\n",
    "import inspect\n",
    "import sys\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import tensorflow_addons as tfa\n",
    "predictions_all = pd.DataFrame()\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score)\n",
    "\n",
    "#used to name the csv file for recording cross validation results and hyperparameters combination\n",
    "results_file = \"bayes_optimization_summary_{}.csv\".format(\n",
    "    strftime(\"%y%m%d%H%M\", gmtime())\n",
    ")\n",
    "\n",
    "# add file header, we will record the cross validation results along with the hyperparameters in a csv file\n",
    "with open(results_file, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"iteration\",\n",
    "            \"loss\",\n",
    "            \"overall_score\",\n",
    "            \"mean_score\",\n",
    "            \"std_score\",\n",
    "            \"min_score\",\n",
    "            \"max_score\",\n",
    "            \"train_time\",\n",
    "            \"params\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    global prediction_all\n",
    "\n",
    "    xgbcl = xgb.XGBClassifier(colsample_bytree =  space['colsample_bytree'], gamma = space['gamma'], learning_rate = space['learning_rate'], max_depth = space['max_depth'], min_child_weight = space['min_child_weight'], n_estimators = space['n_estimators'], reg_alpha = space['reg_alpha'], scale_pos_weight = space['scale_pos_weight'], subsample = space['subsample'], tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "\n",
    "    # execute cross-validation scoring, I will used stratified cross-validation and 3 cross-validation folds\n",
    "    # scoring will be roc_auc since this is an imbalanced dataset\n",
    "    cv = cross_val_score(\n",
    "        estimator=xgbcl,\n",
    "        X=mailout_train.iloc[:,:-1],\n",
    "        y=mailout_train.iloc[:,-1],\n",
    "        verbose=True,\n",
    "        n_jobs=-1,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=False, random_state=None),\n",
    "        scoring='roc_auc',\n",
    "    )\n",
    "    #this fit the XGBClassifier to the MAILOUT TRAIN and fitted model is used to predict on MAILOUT TEST and predictions are concatenated to predictions_all variable\n",
    "    xgbcl.fit(mailout_train.iloc[:,:-1], mailout_train.iloc[:,-1])\n",
    "    predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "    predictions_all = pd.concat([predictions_all, predictions_xgbcl], axis=1)\n",
    "    \n",
    "\n",
    "    # log runtime\n",
    "    run_time = timer() - start\n",
    "    # calculate loss, mean cross validation score, standard deviation and minimum and maximum cross-validation score\n",
    "   \n",
    "    loss = 1 - cv.mean()\n",
    "    mean_score = cv.mean()\n",
    "    std_score = cv.std()\n",
    "    min_score = cv.min()\n",
    "    max_score = cv.max()\n",
    "\n",
    "    # export results to csv\n",
    "    out_file = results_file\n",
    "    with open(out_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                ITERATION,\n",
    "                \n",
    "                loss,\n",
    "                mean_score,\n",
    "                std_score,\n",
    "                min_score,\n",
    "                max_score,\n",
    "                run_time,\n",
    "                space,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return loss\n",
    "\n",
    "#this is the hyperparameter search space for XGBoost Classifier\n",
    "space = { 'learning_rate': hp.uniform('learning_rate', 0.001, 0.050),\n",
    "'max_depth': hp.choice('max_depth', np.arange(3, 10, 1, dtype=int)),\n",
    "'min_child_weight': hp.choice('min_child_weight', np.arange(1, 20, 1, dtype = int)),\n",
    "'gamma': hp.uniform('gamma', 0.04, 0.4),\n",
    "'subsample': hp.uniform('subsample', 0.5, 0.8),\n",
    "'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "'reg_alpha': hp.uniform('reg_alpha', 0.002 , 0.04),\n",
    "'n_estimators': hp.choice('n_estimators', np.arange(400, 600,1, dtype = int)),\n",
    "'scale_pos_weight': hp.choice('scale_pos_weight', np.arange(10, 50, 1, dtype=int)),\n",
    "'tree_method' :'gpu_hist',\n",
    "'sampling_method' : 'gradient_based'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line of codes initiates the bayesian optimisation procedure and I will set maximum iterations at 300\n",
    "ITERATION=0\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=300,\n",
    "    trials=Trials(),\n",
    "    show_progressbar=True,\n",
    ")\n",
    "#this saves all the predictions on MAILOUT TEST to a csv file\n",
    "pd.DataFrame(predictions_all).to_csv('xgbcl model predictions probability on MAILOUT TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following is the hyperparameter combination that provides the best result in Kaggle\n",
    "hyperparam = {'colsample_bytree': 0.5027067891629291, 'gamma': 0.0597643487533388, 'learning_rate': 0.001370272785081644, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 505, 'reg_alpha': 0.005424468447439956, 'sampling_method': 'gradient_based', 'scale_pos_weight': 40, 'subsample': 0.46899673182868984, 'tree_method': 'gpu_hist'}\n",
    "\n",
    "xgbcl4 = xgb.XGBClassifier(colsample_bytree =  0.5027067891629291, gamma = 0.0597643487533388, learning_rate = 0.001370272785081644, max_depth = 4, min_child_weight = 1, n_estimators = 505, reg_alpha = 0.005424468447439956, scale_pos_weight = 40, subsample = 0.46899673182868984, tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "xgbcl4.fit(mailout_train.iloc[:,:-1], mailout_train.iloc[:,-1])\n",
    "\n",
    "import pickle\n",
    "pickle.dump(xgbcl4, open(\"best xgbcl model for training on MAILOUT TRAIN.pkl\" ,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have tried various methods of raising my AUC score in the Kaggle competition and I found that choosing the 130 most important features as predicted by the above best xgbcl\n",
    "#model as features and using AZDIAS + CUSTOMER as the training dataset and MAILOUT TRAIN as the validation dataset, I can achieve the highest AUC score in Kaggle competition\n",
    "\n",
    "#Using the K-Means analysis above for 60 most important features, i will label cluster 0 in AZDIAS as 1 (positive response) and the rest as 0. \n",
    "#while examples in CUSTOMER will all be labelled 0. I choose not to drop any row because I find that not dropping any row raises my AUC score on validation dataset\n",
    "\n",
    "#Therefore we read the raw AZDIAS and CUSTOMER dataset again and perform the same preprocessing and cleaning steps as before\n",
    "train_data = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';', index_col=0)\n",
    "customer_data = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';', index_col=0)\n",
    "\n",
    "#invoking the function that cleans and preprocesses AZDIAS and CUSTOMER dataset\n",
    "# we will not fill missing values with mode/mean since XGBoost can handle missing values\n",
    "train_data = cleaning_data(train_data)\n",
    "customer_data = cleaning_data(customer_data)\n",
    "\n",
    "#we use our best XGBoost model earlier to determine the 130 most important features which will be used as features in the combined AZDIAS and CUSTOMER dataset.\n",
    "\n",
    "#this sorts features from most important to least important\n",
    "sorted = np.argsort(xgbcl4.feature_importances_)[::-1]\n",
    "\n",
    "#this appends the feature names starting from most important to least important to impt_variables list \n",
    "impt_variables = []\n",
    "column_names = train_data.columns \n",
    "for i in sorted:\n",
    "    impt_variables.append(column_names[i])\n",
    "impt_130 = impt_variables[:130]\n",
    "\n",
    "#this converts the ASDIAZ and CUSTOMER data earlier to only include the 130 most important features\n",
    "train_data = train_data[impt_130]\n",
    "customer_data = customer_data[impt_130]\n",
    "\n",
    "#since our earlier K-Means analysis identified cluster 0 as potential customers in AZDIAS, we will label cluster 0 as 1 (positive response)\n",
    "#and the other clusters as 0 (negative response)\n",
    "predicted_label_azdias = pd.DataFrame(predicted_cluster_azdias1).replace([i for i in range(1,10)] ,[0 for i in range(0,9)])\n",
    "predicted_label_azdias = (predicted_label_azdias).replace([0],1)\n",
    "#for CUSTOMER dataset, we will label all examples as 0\n",
    "predicted_label_customer = np.array(customer_data.shape[0])\n",
    "\n",
    "# this concatenate both the AZDIAS and CUSTOMER label\n",
    "predicted_label = pd.concat([pd.DataFrame(predicted_label_azdias), pd.DataFrame(predicted_label_customer)], axis=0)\n",
    "#this concatenate both AZDIAS and CUSTOMER dataset along with the label\n",
    "train_data = pd.concat([train_data, customer_data], axis=0)\n",
    "train_data = pd.concat([train_data.reset_index(drop=True), predicted_label], axis=1)\n",
    "#now we convert the MAILOUT TRAIN to only include the 130 most important features\n",
    "mailout_train = mailout_train[impt_130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Bayesian Optimisation again. For the hyperparameter max_depth, after conducting Bayesian Optimisation for many hundreds iterations, I found that max_depth 16 yields the best AUC score in validation dataset\n",
    "#as such max_depth range will only be set to 16 and as for other hyperparameters, I progressively narrowed my search range by looking up trained models with hyperparameter combinations that yield the best AUC score in both mailout train and mailout test. \n",
    "#As such, I was able to arrive at a hyperparameters combination with max_depth of 16 that produce the highest AUC score in the leaderboard, 0.88149. \n",
    "#I have tried about 30 more times to best my result. However, my effort was futile and I persuaded myself to take satisfaction in my current result. \n",
    "\n",
    "\n",
    "import ast\n",
    "import csv\n",
    "import inspect\n",
    "import sys\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "predictions_all_1 = pd.DataFrame()\n",
    "validation_data = mailout_train.copy()\n",
    "\n",
    "#I will split validation data(MAILOUT TRAIN) into three parts because in the Kaggle competition, it states only 30% of MAILOUT TEST will be used for evaluation\n",
    "# I also have to ensure each part has equal number of RESPONSE of 1 since this is an imbalanced dataset.\n",
    "mailout_train_zeros = validation_data[validation_data['TARGET']==0]\n",
    "mailout_train_ones = validation_data[validation_data['TARGET']==1]\n",
    "mailout_train_sample1 = np.array(pd.concat([mailout_train_zeros.iloc[:13000,:],mailout_train_ones.iloc[:177,:]]).sample(frac=1, random_state =42))\n",
    "mailout_train_sample2 = np.array(pd.concat([mailout_train_zeros.iloc[13000:26000,:],mailout_train_ones.iloc[177:354,:]]).sample(frac=1, random_state =42))\n",
    "mailout_train_sample3 = np.array(pd.concat([mailout_train_zeros.iloc[26000:42962,:],mailout_train_ones.iloc[354:532,:]]).sample(frac=1, random_state =42))\n",
    "validation_data = np.array(validation_data)\n",
    "\n",
    "#used to name the csv file for recording cross validation results and hyperparameters combination\n",
    "results_file = \"bayes_optimization_summary_{}.csv\".format(\n",
    "    strftime(\"%y%m%d%H%M\", gmtime())\n",
    ")\n",
    "\n",
    "# add file header, we will record the cross validation results along with the hyperparameters in a csv file\n",
    "with open(results_file, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"iteration\",\n",
    "            \"loss\",\n",
    "            \"overall_score\",\n",
    "            \"mean_score\",\n",
    "            \"std_score\",\n",
    "            \"min_score\",\n",
    "            \"max_score\",\n",
    "            \"train_time\",\n",
    "            \"params\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "def objective(space):\n",
    "\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    global prediction_all\n",
    "\n",
    "\n",
    "    # used to store the auc scores for all three parts of MAILOUT TRAIN.\n",
    "    auc_score = np.empty(3)\n",
    "    \n",
    "    #training the XGBClassifier on the combined AZDIAS and CUSTOMER dataset with MAILOUT TRAIN as the validation dataset and early stopping rounds set to 40 \n",
    "    eval_set = [(validation_data[:, :-1], validation_data[:, -1])]\n",
    "    xgbcl = xgb.XGBClassifier(colsample_bytree =  space['colsample_bytree'], gamma = space['gamma'], learning_rate = space['learning_rate'], max_depth = space['max_depth'], min_child_weight = space['min_child_weight'], n_estimators = space['n_estimators'], reg_alpha = space['reg_alpha'], scale_pos_weight = space['scale_pos_weight'], subsample = space['subsample'], tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "    xgbcl.fit(np.array(train_data.iloc[:,:-1]), np.array(train_data.iloc[:,-1]), early_stopping_rounds=40, eval_metric=\"auc\", eval_set=eval_set)\n",
    "    \n",
    "    #trained XGBClassifier model is then used to predict on overall MAILOUT TRAIN data and also the 3 separate parts\n",
    "    prediction = (xgbcl.predict_proba(validation_data[:,:-1]))[:,1]\n",
    "    score_a = roc_auc_score(validation_data[:,-1], prediction)\n",
    "    predictions1 = (xgbcl.predict_proba(mailout_train_sample1[:, :-1]))[:, 1]\n",
    "    auc_score[0]=roc_auc_score(mailout_train_sample1[:, -1], predictions1)\n",
    "    predictions2 = (xgbcl.predict_proba(mailout_train_sample2[:, :-1]))[:, 1]\n",
    "    auc_score[1]=roc_auc_score(mailout_train_sample2[:, -1], predictions2)\n",
    "    predictions3 = (xgbcl.predict_proba(mailout_train_sample3[:, :-1]))[:, 1]\n",
    "    auc_score[2]=roc_auc_score(mailout_train_sample3[:, -1], predictions3)\n",
    "    \n",
    "     \n",
    "    #then trained XGBClassifier model is used to predict on MAILOUT TEST and predictions are concatenated to predictions_all_1 variable  \n",
    "    predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "    predictions_all_1 = pd.concat([predictions_all, predictions_xgbcl], axis=1)\n",
    "    \n",
    "   \n",
    "    # log runtime\n",
    "    run_time = timer() - start\n",
    "\n",
    "    # calculate mean loss, mean, min and max auc scores over the 3 parts and overall auc scores on the whole MAILOUT TRAIN dataset\n",
    "\n",
    "    loss = 1 - auc_score.mean()\n",
    "    mean_score = auc_score.mean()\n",
    "    std_score = auc_score.std()\n",
    "    min_score = auc_score.min()\n",
    "    max_score = auc_score.max()\n",
    "    overall_score = score_a\n",
    "\n",
    "    # export results to csv\n",
    "    out_file = results_file\n",
    "    with open(out_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                ITERATION,\n",
    "                loss,\n",
    "                overall_score,\n",
    "                mean_score,\n",
    "                std_score,\n",
    "                min_score,\n",
    "                max_score,\n",
    "                run_time,\n",
    "                space\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I progressively narrowed my search range by looking up trained models with hyperparameter combinations that yield the best AUC score in both mailout train and mailout test. \n",
    "#Below is the refined search space for hyperparameters combination that yield the best AUC score on validation dataset.\n",
    "#As such, I was able to arrive at a hyperparameters combination with max_depth of 16 that produce the highest AUC score in the leaderboard, 0.88149.\n",
    "\n",
    "space = { 'learning_rate': hp.uniform('learning_rate', 0.05, 0.13),\n",
    "'max_depth': hp.choice('max_depth', np.arange(16, 17, 1, dtype=int)),\n",
    "'min_child_weight': hp.choice('min_child_weight', np.arange(5, 9, 1, dtype = int)),\n",
    "'gamma': hp.uniform('gamma', 0.9, 1.1),\n",
    "'subsample': hp.uniform('subsample', 0.8, 1.0),\n",
    "'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 0.85),\n",
    "'reg_alpha': hp.uniform('reg_alpha', 0.17 , 0.23),\n",
    "'n_estimators': hp.choice('n_estimators', np.arange(500, 700,1, dtype = int)),\n",
    "'scale_pos_weight': hp.choice('scale_pos_weight', np.arange(45, 55, 1, dtype=int)),\n",
    "'tree_method' :'gpu_hist',\n",
    "'sampling_method' : 'gradient_based'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line of codes initiates the bayesian optimisation procedure and I will set maximum iterations at 200\n",
    "ITERATION=0\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=200,\n",
    "    trials=Trials(),\n",
    "    show_progressbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following is the hyperparameters combination that provides the best result in Kaggle, namely an AUC of 0.88149\n",
    "hyperparam_best = {'colsample_bytree': 0.776792351030805, 'gamma': 1.0386435777290857, 'learning_rate': 0.10327932030577727, 'max_depth': 16, 'min_child_weight': 5, 'n_estimators': 571, 'reg_alpha': 0.19625707362167116, 'sampling_method': 'gradient_based', 'scale_pos_weight': 48, 'subsample': 0.8894945842180019, 'tree_method': 'gpu_hist'}\n",
    "\n",
    "#training XGBClassifier using the best hyperparameters combination on the combined AZDIAS and CUSTOMER dataset\n",
    "xgbcl5 = xgb.XGBClassifier(colsample_bytree = 0.776792351030805, gamma = 1.0386435777290857, learning_rate = 0.10327932030577727, max_depth = 17, min_child_weight = 5, n_estimators = 571, reg_alpha = 0.19625707362167116, scale_pos_weight = 48, subsample = 0.8894945842180019, tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "# as before i used MAILOUT TRAIN as my validation dataset with early stopping rounds set at 40.\n",
    "eval_set = [(mailout_train[:, :-1], mailout_train[:, -1])]\n",
    "xgbcl5.fit(np.array(train_data.iloc[:,:-1]), np.array(train_data.iloc[:,-1]), early_stopping_rounds=40, eval_metric=\"auc\", eval_set=eval_set)\n",
    "#find the AUC for predictions on MAILOUT TRAIN\n",
    "prediction = (xgbcl5.predict_proba(mailout_train[:,:-1]))[:,1]\n",
    "score_auc = roc_auc_score(mailout_train[:,-1], prediction)\n",
    "\n",
    "#use the trained XGBClassifier model to predict on MAILOUT TEST and which will be submitted to Kaggle competition\n",
    "#convert MAILOUT TEST to only include the 130 most important features\n",
    "mailout_test = mailout_test[impt_130]\n",
    "\n",
    "#would need to obtain the LNR column for submission to Kaggle\n",
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', index_col=0)\n",
    "mailout_test_lnr = mailout_test['LNR']\n",
    "\n",
    "#predict on the MAILOUT TEST\n",
    "predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "#concatenate predictions with LNR for submission to Kaggle\n",
    "predictions_xgbcl = pd.concat([mailout_test_lnr, predictions_xgbcl], axis = 1, ignore_index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
