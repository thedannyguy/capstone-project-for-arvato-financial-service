{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for customer columns not in general pop column\n",
    "customer_col = [i for i in customer_data.columns.to_list() if i not in train_data.columns.to_list()]\n",
    "to_drop = ['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP']\n",
    "train_data.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "\n",
    "#create a dataframe showing number of missing values for each feature\n",
    "null_columns = (np.count_nonzero(train_data[train_data.columns].isnull(), axis = 0))\n",
    "null_columns_df = pd.DataFrame(null_columns,index=train_data.columns)\n",
    "null_columns_df = null_columns_df.sort_values(0,ascending=False)\n",
    "null_columns_df = null_columns_df.rename(columns={0:'no of missing values'})\n",
    "\n",
    "#poltting bar graph for features with 17 most missing values\n",
    "import matplotlib.pyplot as plt   \n",
    "Features = null_columns_df.index[0:17].to_list()[::-1]\n",
    "Quantity = null_columns_df['no of missing values'][0:17].to_list()[::-1]\n",
    "\n",
    "plt.barh(Features,Quantity)\n",
    "plt.title('FEATURES WITH MISSING VALUES')\n",
    "plt.ylabel('FEATURES')\n",
    "plt.xlabel('NO OF MISSING VALUES')\n",
    "plt.show()\n",
    "\n",
    "#create a dictionary with features as key and number of missing values as value\n",
    "column_names = train_data.columns\n",
    "null_info =dict()\n",
    "for i, j in zip(null_columns, column_names):\n",
    "    if i > 0:\n",
    "        null_info[j] = i\n",
    "\n",
    "null_info_sorted = dict(sorted(null_info.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#drop columns with 400000 or more missing values\n",
    "columns_to_drop = [i for i in null_info if null_info[i] >= 400000]\n",
    "train_data.drop(columns = columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "#replace unknown values (-1, 0, 9) with nan by searching for unknown values in diss_attribute\n",
    "var_unknown = {}\n",
    "dias_att['Value'][0]\n",
    "for i,j,k in zip(dias_att['Attribute'], dias_att['Meaning'], dias_att['Value']):\n",
    "    if i != np.nan and j == 'unknown':\n",
    "        if type(k) == str:\n",
    "            val = [int(i) for i in k.split(\",\")]\n",
    "\n",
    "            var_unknown[i] = val\n",
    "        else:\n",
    "            var_unknown[i] = k\n",
    "\n",
    "#search for features that are in dias attribute but not in azdias dataset\n",
    "var_unknown_col = [i for i in var_unknown if i not in train_data.columns.to_list()]\n",
    "\n",
    "#rename columns\n",
    "columns_to_rename = ['CAMEO_DEUINTL_2015', 'KBA13_CCM_1400_2500', 'SOHO_FLAG', np.nan]\n",
    "columns_renamed = ['CAMEO_INTL_2015', 'KBA13_CCM_1401_2500', 'SOHO_KZ']\n",
    "columns_to_remove = ['BIP_FLAG', 'D19_KK_KUNDENTYP', 'GEOSCORE_KLS7', 'HAUSHALTSSTRUKTUR', 'WACHSTUMSGEBIET_NB']\n",
    "\n",
    "k = 0\n",
    "for i,j in var_unknown.items():\n",
    "    if i in columns_to_remove:\n",
    "        continue\n",
    "    if i in columns_to_rename:\n",
    "        train_data[columns_renamed[k]] = train_data[columns_renamed[k]].replace(j, np.nan)\n",
    "        continue\n",
    "    train_data[i] = train_data[i].replace(j, np.nan)\n",
    "\n",
    "#check if -1 values have been replaced\n",
    "(np.count_nonzero(train_data[train_data.columns]==-1, axis = 0))\n",
    "\n",
    "#for value 9, there are features with value 9 that is unknown. Therefore i manually check each feature that have value 9.\n",
    "val_9 = (np.count_nonzero(train_data[train_data.columns]==9, axis = 0))\n",
    "column_names = train_data.columns\n",
    "val_9_1 ={}\n",
    "for i, j in zip(val_9, column_names):\n",
    "    if i > 0:\n",
    "        val_9_1[j] = i\n",
    "#after checking, all features with unknown values -1 and 9 have been converted into NaN.\n",
    "\n",
    "#'RELAT_AB' need to replace 9 with nan since the azdias attribute did not list the unknown in the first columm of the feature\n",
    "train_data['RELAT_AB'] = train_data['RELAT_AB'].replace([9], np.nan)\n",
    "\n",
    "#check columns with nan values again\n",
    "null_columns_1 = (np.count_nonzero(train_data[train_data.columns].isnull(), axis = 0))\n",
    "null_columns_df_1 = pd.DataFrame(null_columns_1,index=train_data.columns)\n",
    "null_columns_df_1 = null_columns_df_1.sort_values(0,ascending=False)\n",
    "null_columns_df_1 = null_columns_df_1.rename(columns={0:'no of missing values'})\n",
    "\n",
    "#plotting bar graph for top 5 features with missing values\n",
    "Features1 = null_columns_df_1.index[0:5].to_list()[::-1]\n",
    "Quantity1 = null_columns_df_1['no of missing values'][0:5].to_list()[::-1]\n",
    "\n",
    "plt.barh(Features1,Quantity1)\n",
    "plt.title('FEATURES WITH MISSING VALUES')\n",
    "plt.ylabel('FEATURES')\n",
    "plt.xlabel('NO OF MISSING VALUES')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#create a dictionary with features as key and number of missing values as value\n",
    "null_info_1 ={}\n",
    "for i, j in zip(null_columns_1, column_names):\n",
    "    if i > 0:\n",
    "        null_info_1[j] = i\n",
    "null_info_sorted_1 = dict(sorted(null_info_1.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#drop columns with 476524 or more missing values\n",
    "columns_to_drop_1 = [i for i in null_info_1 if null_info_1[i] >= 476524]\n",
    "train_data.drop(columns = columns_to_drop_1, inplace=True)\n",
    "\n",
    "#determine the features that are in AZDIAS dataset but not in DIAS attribute\n",
    "train_data_features_not_in_attributes = [i for i in train_data.columns.to_list() if i not in dias_att['Attribute'].to_list()]\n",
    "len(train_data_features_not_in_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FURTHER DATA CLEANING\n",
    "\n",
    "#dropping 'LP_STATUS_GROB', 'LP_FAMILIE_GROB' because similar to 'LP_STATUS_FEIN' and 'LP_FAMILIE_FEIN' \n",
    "train_data.drop(columns = ['LP_STATUS_GROB', 'LP_FAMILIE_GROB'], inplace=True)\n",
    "#dropping LNR because it is a list of identification numbers\n",
    "train_data.drop(columns = ['LNR'], inplace=True)\n",
    "\n",
    "#reengineering features\n",
    "#PLZ8_BAUMAX, how many family homes\n",
    "train_data['PLZ8_BAUMAX'] = train_data['PLZ8_BAUMAX'].replace([5], 0)\n",
    "#if person lives in business building or not\n",
    "train_data['PLZ8_BAUMAX_BIZ'] = train_data['PLZ8_BAUMAX'].replace([0,1,2,3,4], [1,0,0,0,0])\n",
    "#mainstream or avant-garde movement\n",
    "train_data['PRAEGENDE_MAIN_OR_AVANT'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [0,1,0,1,0,1,1,0,1,0,1,0,1,0,1])\n",
    "#replace 40ies,50ies,60ies,70ies,80ies,90ies movements with ordinal numbers\n",
    "train_data['PRAEGENDE_JUGENDJAHRE'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [1,1,2,2,3,3,3,4,4,5,5,5,5,6,6])\n",
    "\n",
    "#CAMEO_INTL_2015 and CAMEO_DEUG_2015 have string values ('X' and 'XX') that need to be replaced with NaN.\n",
    "train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].replace(['X', 'XX'], np.nan)\n",
    "train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].astype(float)\n",
    "train_data['CAMEO_DEUG_2015'] = train_data['CAMEO_DEUG_2015'].replace(['X', 'XX'], np.nan)\n",
    "train_data['CAMEO_DEUG_2015']  = train_data['CAMEO_DEUG_2015'].astype(float)\n",
    "\n",
    "#creating a new feature indicating household wealth status from 'CAMEO HOUSEHOLD'\n",
    "train_data['CAMEO_HOUSEHOLD'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                              41, 42, 43, 44, 45, 51, 52, 53, 54, 55],[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5])\n",
    "#creating a new feature indicating family type from 'CAMEO HOUSEHOLD' \n",
    "train_data['CAMEO_FAMILY_TYPE'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "\n",
    "#dropping CAMEO_INTL_2015                                                                          \n",
    "train_data.drop(columns = ['CAMEO_INTL_2015'], inplace=True)                                                                             41, 42, 43, 44, 45, 51, 52, 53, 54, 55], [1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5])\n",
    "#OST_WEST_KZ encode 'W' to 0 and '0' to 1\n",
    "train_data['OST_WEST_KZ'] = train_data['OST_WEST_KZ'].replace(['W', 'O'], [0,1]) \n",
    "#one hot encoding of 'ANREDE_KZ' into 0 for male and 1 for female\n",
    "train_data = pd.get_dummies(train_data, columns=['ANREDE_KZ'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for WOHNLAGE, replace 0 with nan\n",
    "train_data['WOHNLAGE'] = train_data['WOHNLAGE'].replace([0], np.nan)\n",
    "#create new WOHNLAGE feature, urban or rural region\n",
    "train_data['WOHNLAGE_REGION'] = train_data['WOHNLAGE'].replace([1,2,3,4,5,7,8],[0,0,0,0,0,1,1])\n",
    "\n",
    "#dropping CAMEO_DEU, because they are nominal features and have too many categories (44 categoris) \n",
    "train_data.drop(columns = ['CAMEO_DEU_2015'], inplace=True)\n",
    "#dropping LP_LEBENSPHASE_FEIN, because they are nominal features and have too many categories (40 categoris)\n",
    "train_data.drop(columns = ['LP_LEBENSPHASE_FEIN'], inplace=True)\n",
    "#dropping D19_LETZTER_KAUF_BRANCHE, since it is duplicated information\n",
    "train_data.drop(columns=['D19_LETZTER_KAUF_BRANCHE'], inplace=True)\n",
    "\n",
    "\n",
    "#converting 'EINGEFUEGT_AM' to total months (1/1/2019 is used as the end date)\n",
    "eingefuegt = np.empty(shape=(train_data.shape[0],1))\n",
    "k = 0\n",
    "train_data['EINGEFUEGT_AM'] = train_data['EINGEFUEGT_AM'].fillna(-1)\n",
    "for i in train_data['EINGEFUEGT_AM']:\n",
    "    if i != -1:\n",
    "\n",
    "        yrmthday, zeros = i.split()\n",
    "        mth_1 = (2019 - (int(yrmthday[0:4]) + 1)) * 12\n",
    "        mth_2 = 12 - int(yrmthday[5:6])\n",
    "        mth_3 = (30 - int(yrmthday[8:10])) / 30\n",
    "        mth_total = mth_1 + mth_2 + mth_3\n",
    "        eingefuegt[k] = mth_total\n",
    "    else:\n",
    "        eingefuegt[k] = -1\n",
    "    k += 1\n",
    "    \n",
    "        \n",
    "#remove 'EINGEFUEGT_AM' and create a new feature 'EINGEFUEGT_MODIFIED'\n",
    "train_data.drop(columns =['EINGEFUEGT_AM'] , inplace=True)\n",
    "train_data['EINGEFUEGT_MODIFIED'] = pd.DataFrame(eingefuegt)\n",
    "#since i converted the NaN values to -1 earlier, i will need to replace the -1 value with mode value\n",
    "train_data['EINGEFUEGT_MODIFIED'] = train_data['EINGEFUEGT_MODIFIED'].replace([-1],train_data['EINGEFUEGT_MODIFIED'].mode().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function fills NaN values with mean value for continuous features and mode value for nominal/ordinal features\n",
    "def impute_nan_values(df):  \n",
    "    continuous_features = ['EINGEZOGENAM_HH_JAHR' , 'MIN_GEBAEUDEJAHR', 'KBA13_ANZAHL_PKW', 'ANZ_HAUSHALTE_AKTIV','VERDICHTUNGSRAUM', 'ANZ_STATISTISCHE_HAUSHALTE']\n",
    "    \n",
    "    #fill continuous features nan with mean value\n",
    "    df[continuous_features] = df[continuous_features].fillna(np.mean(df[continuous_features]))\n",
    "\n",
    "    #list of categorical/ordinal features\n",
    "    all_features_except_continuous = df.columns.to_list()\n",
    "    for i in continuous_features:\n",
    "        all_features_except_continuous.remove(i)\n",
    "\n",
    "    #fill categorical features nan with most frequent values\n",
    "    for column in df[all_features_except_continuous].columns:\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#fill AZDIAS and CUSTOMER NaN values with mode/mean values\n",
    "train_data = impute_nan_values(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
