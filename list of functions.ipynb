{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function fills NaN values with mean value for continuous features and mode value for nominal/ordinal features\n",
    "\n",
    "def impute_nan_values(df):  \n",
    "    continuous_features = ['EINGEZOGENAM_HH_JAHR' , 'MIN_GEBAEUDEJAHR', 'KBA13_ANZAHL_PKW', 'ANZ_HAUSHALTE_AKTIV','VERDICHTUNGSRAUM', 'ANZ_STATISTISCHE_HAUSHALTE']\n",
    "    \n",
    "    #fill continuous features nan with mean value\n",
    "    df[continuous_features] = df[continuous_features].fillna(np.mean(df[continuous_features]))\n",
    "\n",
    "    #list of categorical/ordinal features\n",
    "    all_features_except_continuous = df.columns.to_list()\n",
    "    for i in continuous_features:\n",
    "        all_features_except_continuous.remove(i)\n",
    "\n",
    "    #fill categorical features nan with most frequent values\n",
    "    for column in df[all_features_except_continuous].columns:\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(train_data):\n",
    "    columns_to_drop = ['ALTER_KIND1', 'ALTER_KIND2', 'ALTER_KIND3', 'ALTER_KIND4', 'EXTSEL992', 'KK_KUNDENTYP']\n",
    "    train_data.drop(columns = columns_to_drop, inplace=True)\n",
    "    k = 0\n",
    "    for i,j in var_unknown.items():\n",
    "        if i in columns_to_remove:\n",
    "            continue\n",
    "        if i in columns_to_rename:\n",
    "            train_data[columns_renamed[k]] = train_data[columns_renamed[k]].replace(j, np.nan)\n",
    "            continue\n",
    "        train_data[i] = train_data[i].replace(j, np.nan)\n",
    "    #'RELAT_AB' need to replace 9 with nan\n",
    "    train_data['RELAT_AB'] = train_data['RELAT_AB'].replace([9], np.nan)\n",
    "    columns_to_drop_1 = ['AGER_TYP', 'KBA05_BAUMAX', 'TITEL_KZ']\n",
    "    train_data.drop(columns = columns_to_drop_1, inplace=True)\n",
    "\n",
    "    #dropping 'LP_STATUS_GROB', 'LP_FAMILIE_GROB' because similar to 'LP_STATUS_FEIN' and 'LP_FAMILIE_FEIN' \n",
    "    train_data.drop(columns = ['LP_STATUS_GROB', 'LP_FAMILIE_GROB'], inplace=True)\n",
    "    #dropping LNR\n",
    "    train_data.drop(columns = ['LNR'], inplace=True)\n",
    "\n",
    "    #reengineering features\n",
    "    #PLZ8_BAUMAX, how many family homes\n",
    "    train_data['PLZ8_BAUMAX'] = train_data['PLZ8_BAUMAX'].replace([5], 0)\n",
    "    #if person lives in business building or not\n",
    "    train_data['PLZ8_BAUMAX_BIZ'] = train_data['PLZ8_BAUMAX'].replace([0,1,2,3,4], [1,0,0,0,0])\n",
    "    #main of avant-garde\n",
    "    train_data['PRAEGENDE_MAIN_OR_AVANT'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [0,1,0,1,0,1,1,0,1,0,1,0,1,0,1])\n",
    "    #40ies,50ies,60ies,70ies,80ies,90ies movements\n",
    "    train_data['PRAEGENDE_JUGENDJAHRE'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [1,1,2,2,3,3,3,4,4,5,5,5,5,6,6])\n",
    "\n",
    "    #CAMEO_INTL\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].astype(float)\n",
    "    train_data['CAMEO_DEUG_2015'] = train_data['CAMEO_DEUG_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_DEUG_2015']  = train_data['CAMEO_DEUG_2015'].astype(float)\n",
    "    #household income\n",
    "    train_data['CAMEO_HOUSEHOLD'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                  41, 42, 43, 44, 45, 51, 52, 53, 54, 55],[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5])\n",
    "    #family type \n",
    "    train_data['CAMEO_FAMILY_TYPE'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                   41, 42, 43, 44, 45, 51, 52, 53, 54, 55], [1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5])\n",
    "\n",
    "    #OST_WEST_KZ encode 'W' to 0 and '0' to 1\n",
    "    train_data['OST_WEST_KZ'] = train_data['OST_WEST_KZ'].replace(['W', 'O'], [0,1]) \n",
    "    #one hot encoding of 'ANREDE_KZ'\n",
    "    train_data = pd.get_dummies(train_data, columns=['ANREDE_KZ'], drop_first=True)\n",
    "\n",
    "\n",
    "    train_data.drop(columns = ['CAMEO_INTL_2015'], inplace=True)\n",
    "    #for WOHNLAGE, replace 0 with nan\n",
    "    train_data['WOHNLAGE'] = train_data['WOHNLAGE'].replace([0], np.nan)\n",
    "    #create new WOHNLAGE feature, urban or rural\n",
    "    train_data['WOHNLAGE_REGION'] = train_data['WOHNLAGE'].replace([1,2,3,4,5,7,8],[0,0,0,0,0,1,1])\n",
    "\n",
    "    #dropping CAMEO_DEU, too many categories \n",
    "    train_data.drop(columns = ['CAMEO_DEU_2015'], inplace=True)\n",
    "    #dropping LP_LEBENSPHASE_FEIN, too many categories\n",
    "    train_data.drop(columns = ['LP_LEBENSPHASE_FEIN'], inplace=True)\n",
    "    #dropping D19_LETZTER_KAUF_BRANCHE, duplicated information\n",
    "    train_data.drop(columns=['D19_LETZTER_KAUF_BRANCHE'], inplace=True)\n",
    "\n",
    "    #converting 'EINGEFUEGT_AM' to total months\n",
    "    eingefuegt = np.empty(shape=(train_data.shape[0],1))\n",
    "    k = 0\n",
    "    train_data['EINGEFUEGT_AM'] = train_data['EINGEFUEGT_AM'].fillna(-1)\n",
    "    for i in train_data['EINGEFUEGT_AM']:\n",
    "        if i != -1:\n",
    "\n",
    "            yrmthday, zeros = i.split()\n",
    "            mth_1 = (2019 - (int(yrmthday[0:4]) + 1)) * 12\n",
    "            mth_2 = 12 - int(yrmthday[5:6])\n",
    "            mth_3 = (30 - int(yrmthday[8:10])) / 30\n",
    "            mth_total = mth_1 + mth_2 + mth_3\n",
    "            eingefuegt[k] = mth_total\n",
    "        else:\n",
    "            eingefuegt[k] = -1\n",
    "        k += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data.drop(columns =['EINGEFUEGT_AM'] , inplace=True)\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = pd.DataFrame(eingefuegt)\n",
    "\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = train_data['EINGEFUEGT_MODIFIED'].replace([-1],train_data['EINGEFUEGT_MODIFIED'].mode().values)\n",
    "    \n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before that PCA analysis is still useful for identifying interesting features, so we will conduct PCA here\n",
    "def pca_fit(df):\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import normalize\n",
    "    #normalize data\n",
    "    df = normalize(df)\n",
    "    #fit PCA on data\n",
    "    pca = PCA().fit(df)\n",
    "    \n",
    "    #only select components that cumulatively exceed 0.9 explaiend variance ratio\n",
    "    k = 0\n",
    "    m = 1\n",
    "    for i in pca.explained_variance_ratio_:\n",
    "        k += i\n",
    "        if k > 0.9:\n",
    "            break\n",
    "        m += 1\n",
    "\n",
    "    no_comp = m\n",
    "    \n",
    "    #fit PCA again on data with the desired number of component\n",
    "    df = PCA(n_components=m).fit_transform(df)\n",
    "    return pca, df\n",
    "\n",
    "pca, train_data_transformed = pca_fit(train_data)\n",
    "\n",
    "#Next we will find interesting variables identified by PCA\n",
    "\n",
    "# the set should contain the variable's string name (as found in X.colummns)\n",
    "def find_interesting_variables(pca, length_pca_component, top_length, feature_names):\n",
    "    interesting_variables = set()\n",
    " \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import normalize\n",
    "    \n",
    "    #how many PCA component to analyse\n",
    "    component = pca.components_[0:length_pca_component]\n",
    "    \n",
    "    #how many interesting variables sorted by most interesting to least interesting to return\n",
    "    for i in range(len(component)):\n",
    "    \n",
    "        feature_with_magnitude = dict(zip(feature_names, component))\n",
    "        sorted_feature_with_magnitude = sorted(feature_with_magnitude.items(), key=lambda item: item[1])\n",
    "        j = 0\n",
    "        for k,l in  sorted_feature_with_magnitude.items():\n",
    "            if j == top_length:\n",
    "                break\n",
    "            interesting_variables.add(k)\n",
    "            j += 1\n",
    "                    \n",
    "    return interesting_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to fit K-Means algortihm on dataset and then predict the clusters that dataset belong to\n",
    "def kmeans_segment(df, df2, no_cluster):\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=no_cluster , n_init=no_cluster+3, verbose=3, max_iter=500)\n",
    "    km.fit(df)\n",
    "    predicted_cluster_azdias = km.predict(df)\n",
    "    predicted_cluster_customer = km.predict(df2)\n",
    "    \n",
    "    return predicted_cluster_azdias, predicted_cluster_customer\n",
    "\n",
    "#function to count how many examples are contained in each cluster\n",
    "def segmentation_count(array, count):\n",
    "    segment_count = {}\n",
    "    for i in range(count):\n",
    "        segment_count[i] = np.count_nonzero(array==i)\n",
    "    return segment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    global prediction_all\n",
    "\n",
    "    xgbcl = xgb.XGBClassifier(colsample_bytree =  space['colsample_bytree'], gamma = space['gamma'], learning_rate = space['learning_rate'], max_depth = space['max_depth'], min_child_weight = space['min_child_weight'], n_estimators = space['n_estimators'], reg_alpha = space['reg_alpha'], scale_pos_weight = space['scale_pos_weight'], subsample = space['subsample'], tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "\n",
    "    # execute cross-validation scoring, I will used stratified cross-validation and 3 cross-validation folds\n",
    "    # scoring will be roc_auc since this is an imbalanced dataset\n",
    "    cv = cross_val_score(\n",
    "        estimator=xgbcl,\n",
    "        X=mailout_train.iloc[:,:-1],\n",
    "        y=mailout_train.iloc[:,-1],\n",
    "        verbose=True,\n",
    "        n_jobs=-1,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=False, random_state=None),\n",
    "        scoring='roc_auc',\n",
    "    )\n",
    "    #this fit the XGBClassifier to the MAILOUT TRAIN and fitted model is used to predict on MAILOUT TEST and predictions are concatenated to predictions_all variable\n",
    "    xgbcl.fit(mailout_train.iloc[:,:-1], mailout_train.iloc[:,-1])\n",
    "    predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "    predictions_all = pd.concat([predictions_all, predictions_xgbcl], axis=1)\n",
    "    \n",
    "\n",
    "    # log runtime\n",
    "    run_time = timer() - start\n",
    "    # calculate loss, mean cross validation score, standard deviation and minimum and maximum cross-validation score\n",
    "   \n",
    "    loss = 1 - cv.mean()\n",
    "    mean_score = cv.mean()\n",
    "    std_score = cv.std()\n",
    "    min_score = cv.min()\n",
    "    max_score = cv.max()\n",
    "\n",
    "    # export results to csv\n",
    "    out_file = results_file\n",
    "    with open(out_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                ITERATION,\n",
    "                \n",
    "                loss,\n",
    "                mean_score,\n",
    "                std_score,\n",
    "                min_score,\n",
    "                max_score,\n",
    "                run_time,\n",
    "                space,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    global prediction_all\n",
    "\n",
    "\n",
    "    # used to store the auc scores for all three parts of MAILOUT TRAIN.\n",
    "    auc_score = np.empty(3)\n",
    "    \n",
    "    #training the XGBClassifier on the combined AZDIAS and CUSTOMER dataset with MAILOUT TRAIN as the validation dataset and early stopping rounds set to 40 \n",
    "    eval_set = [(validation_data[:, :-1], validation_data[:, -1])]\n",
    "    xgbcl = xgb.XGBClassifier(colsample_bytree =  space['colsample_bytree'], gamma = space['gamma'], learning_rate = space['learning_rate'], max_depth = space['max_depth'], min_child_weight = space['min_child_weight'], n_estimators = space['n_estimators'], reg_alpha = space['reg_alpha'], scale_pos_weight = space['scale_pos_weight'], subsample = space['subsample'], tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "    xgbcl.fit(np.array(train_data.iloc[:,:-1]), np.array(train_data.iloc[:,-1]), early_stopping_rounds=40, eval_metric=\"auc\", eval_set=eval_set)\n",
    "    \n",
    "    #trained XGBClassifier model is then used to predict on overall MAILOUT TRAIN data and also the 3 separate parts\n",
    "    prediction = (xgbcl.predict_proba(validation_data[:,:-1]))[:,1]\n",
    "    score_a = roc_auc_score(validation_data[:,-1], prediction)\n",
    "    predictions1 = (xgbcl.predict_proba(mailout_train_sample1[:, :-1]))[:, 1]\n",
    "    auc_score[0]=roc_auc_score(mailout_train_sample1[:, -1], predictions1)\n",
    "    predictions2 = (xgbcl.predict_proba(mailout_train_sample2[:, :-1]))[:, 1]\n",
    "    auc_score[1]=roc_auc_score(mailout_train_sample2[:, -1], predictions2)\n",
    "    predictions3 = (xgbcl.predict_proba(mailout_train_sample3[:, :-1]))[:, 1]\n",
    "    auc_score[2]=roc_auc_score(mailout_train_sample3[:, -1], predictions3)\n",
    "    \n",
    "     \n",
    "    #then trained XGBClassifier model is used to predict on MAILOUT TEST and predictions are concatenated to predictions_all_1 variable  \n",
    "    predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "    predictions_all_1 = pd.concat([predictions_all, predictions_xgbcl], axis=1)\n",
    "    \n",
    "   \n",
    "    # log runtime\n",
    "    run_time = timer() - start\n",
    "\n",
    "    # calculate mean loss, mean, min and max auc scores over the 3 parts and overall auc scores on the whole MAILOUT TRAIN dataset\n",
    "\n",
    "    loss = 1 - auc_score.mean()\n",
    "    mean_score = auc_score.mean()\n",
    "    std_score = auc_score.std()\n",
    "    min_score = auc_score.min()\n",
    "    max_score = auc_score.max()\n",
    "    overall_score = score_a\n",
    "\n",
    "    # export results to csv\n",
    "    out_file = results_file\n",
    "    with open(out_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                ITERATION,\n",
    "                loss,\n",
    "                overall_score,\n",
    "                mean_score,\n",
    "                std_score,\n",
    "                min_score,\n",
    "                max_score,\n",
    "                run_time,\n",
    "                space\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
