{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "#azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "#customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to add in a lot more cells (both markdown and code) to document your\n",
    "# approach and findings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to pip install xlrd for it to read DIAS attribute excel file\n",
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#load in the data\n",
    "dias_att = pd.read_excel('/home/workspace/DIAS Information Levels - Attributes 2017.xlsx')\n",
    "train_data = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';', index_col=0)\n",
    "customer_data = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';', index_col=0)\n",
    "train_data = pd.concat([train_data, customer_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#check for customer columns not in general pop column\n",
    "customer_col = [i for i in customer_data.columns.to_list() if i not in train_data.columns.to_list()]\n",
    "to_drop = ['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP']\n",
    "train_data.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "\n",
    "#create a dataframe showing number of missing values for each feature\n",
    "null_columns = (np.count_nonzero(train_data[train_data.columns].isnull(), axis = 0))\n",
    "null_columns_df = pd.DataFrame(null_columns,index=train_data.columns)\n",
    "null_columns_df = null_columns_df.sort_values(0,ascending=False)\n",
    "null_columns_df = null_columns_df.rename(columns={0:'no of missing values'})\n",
    "\n",
    "#poltting bar graph for features with 17 most missing values\n",
    "import matplotlib.pyplot as plt   \n",
    "Features = null_columns_df.index[0:17].to_list()[::-1]\n",
    "Quantity = null_columns_df['no of missing values'][0:17].to_list()[::-1]\n",
    "\n",
    "plt.barh(Features,Quantity)\n",
    "plt.title('FEATURES WITH MISSING VALUES')\n",
    "plt.ylabel('FEATURES')\n",
    "plt.xlabel('NO OF MISSING VALUES')\n",
    "plt.show()\n",
    "\n",
    "#create a dictionary with features as key and number of missing values as value\n",
    "column_names = train_data.columns\n",
    "null_info =dict()\n",
    "for i, j in zip(null_columns, column_names):\n",
    "    if i > 0:\n",
    "        null_info[j] = i\n",
    "\n",
    "null_info_sorted = dict(sorted(null_info.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#drop columns with 400000 or more missing values\n",
    "columns_to_drop = [i for i in null_info if null_info[i] >= 400000]\n",
    "train_data.drop(columns = columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "#replace unknown values (-1, 0, 9) with nan by searching for unknown values in diss_attribute\n",
    "var_unknown = {}\n",
    "dias_att['Value'][0]\n",
    "for i,j,k in zip(dias_att['Attribute'], dias_att['Meaning'], dias_att['Value']):\n",
    "    if i != np.nan and j == 'unknown':\n",
    "        if type(k) == str:\n",
    "            val = [int(i) for i in k.split(\",\")]\n",
    "\n",
    "            var_unknown[i] = val\n",
    "        else:\n",
    "            var_unknown[i] = k\n",
    "\n",
    "#search for features that are in dias attribute but not in azdias dataset\n",
    "var_unknown_col = [i for i in var_unknown if i not in train_data.columns.to_list()]\n",
    "\n",
    "#rename columns\n",
    "columns_to_rename = ['CAMEO_DEUINTL_2015', 'KBA13_CCM_1400_2500', 'SOHO_FLAG', np.nan]\n",
    "columns_renamed = ['CAMEO_INTL_2015', 'KBA13_CCM_1401_2500', 'SOHO_KZ']\n",
    "columns_to_remove = ['BIP_FLAG', 'D19_KK_KUNDENTYP', 'GEOSCORE_KLS7', 'HAUSHALTSSTRUKTUR', 'WACHSTUMSGEBIET_NB']\n",
    "\n",
    "k = 0\n",
    "for i,j in var_unknown.items():\n",
    "    if i in columns_to_remove:\n",
    "        continue\n",
    "    if i in columns_to_rename:\n",
    "        train_data[columns_renamed[k]] = train_data[columns_renamed[k]].replace(j, np.nan)\n",
    "        continue\n",
    "    train_data[i] = train_data[i].replace(j, np.nan)\n",
    "\n",
    "#check if -1 values have been replaced\n",
    "(np.count_nonzero(train_data[train_data.columns]==-1, axis = 0))\n",
    "\n",
    "#for value 9, there are features with value 9 that is unknown. Therefore i manually check each feature that have value 9.\n",
    "val_9 = (np.count_nonzero(train_data[train_data.columns]==9, axis = 0))\n",
    "column_names = train_data.columns\n",
    "val_9_1 ={}\n",
    "for i, j in zip(val_9, column_names):\n",
    "    if i > 0:\n",
    "        val_9_1[j] = i\n",
    "#after checking, all features with unknown values -1 and 9 have been converted into NaN.\n",
    "\n",
    "#'RELAT_AB' need to replace 9 with nan since the azdias attribute did not list the unknown in the first columm of the feature\n",
    "train_data['RELAT_AB'] = train_data['RELAT_AB'].replace([9], np.nan)\n",
    "\n",
    "#check columns with nan values again\n",
    "null_columns_1 = (np.count_nonzero(train_data[train_data.columns].isnull(), axis = 0))\n",
    "null_columns_df_1 = pd.DataFrame(null_columns_1,index=train_data.columns)\n",
    "null_columns_df_1 = null_columns_df_1.sort_values(0,ascending=False)\n",
    "null_columns_df_1 = null_columns_df_1.rename(columns={0:'no of missing values'})\n",
    "\n",
    "#plotting bar graph for top 5 features with missing values\n",
    "Features1 = null_columns_df_1.index[0:5].to_list()[::-1]\n",
    "Quantity1 = null_columns_df_1['no of missing values'][0:5].to_list()[::-1]\n",
    "\n",
    "plt.barh(Features1,Quantity1)\n",
    "plt.title('FEATURES WITH MISSING VALUES')\n",
    "plt.ylabel('FEATURES')\n",
    "plt.xlabel('NO OF MISSING VALUES')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#create a dictionary with features as key and number of missing values as value\n",
    "null_info_1 ={}\n",
    "for i, j in zip(null_columns_1, column_names):\n",
    "    if i > 0:\n",
    "        null_info_1[j] = i\n",
    "null_info_sorted_1 = dict(sorted(null_info_1.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#drop columns with 476524 or more missing values\n",
    "columns_to_drop_1 = [i for i in null_info_1 if null_info_1[i] >= 476524]\n",
    "train_data.drop(columns = columns_to_drop_1, inplace=True)\n",
    "\n",
    "#determine the features that are in AZDIAS dataset but not in DIAS attribute\n",
    "train_data_features_not_in_attributes = [i for i in train_data.columns.to_list() if i not in dias_att['Attribute'].to_list()]\n",
    "len(train_data_features_not_in_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FURTHER DATA CLEANING\n",
    "\n",
    "#dropping 'LP_STATUS_GROB', 'LP_FAMILIE_GROB' because similar to 'LP_STATUS_FEIN' and 'LP_FAMILIE_FEIN' \n",
    "train_data.drop(columns = ['LP_STATUS_GROB', 'LP_FAMILIE_GROB'], inplace=True)\n",
    "#dropping LNR because it is a list of identification numbers\n",
    "train_data.drop(columns = ['LNR'], inplace=True)\n",
    "\n",
    "#reengineering features\n",
    "#PLZ8_BAUMAX, how many family homes\n",
    "train_data['PLZ8_BAUMAX'] = train_data['PLZ8_BAUMAX'].replace([5], 0)\n",
    "#if person lives in business building or not\n",
    "train_data['PLZ8_BAUMAX_BIZ'] = train_data['PLZ8_BAUMAX'].replace([0,1,2,3,4], [1,0,0,0,0])\n",
    "#mainstream or avant-garde movement\n",
    "train_data['PRAEGENDE_MAIN_OR_AVANT'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [0,1,0,1,0,1,1,0,1,0,1,0,1,0,1])\n",
    "#replace 40ies,50ies,60ies,70ies,80ies,90ies movements with ordinal numbers\n",
    "train_data['PRAEGENDE_JUGENDJAHRE'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [1,1,2,2,3,3,3,4,4,5,5,5,5,6,6])\n",
    "\n",
    "#CAMEO_INTL_2015 and CAMEO_DEUG_2015 have string values ('X' and 'XX') that need to be replaced with NaN.\n",
    "train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].replace(['X', 'XX'], np.nan)\n",
    "train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].astype(float)\n",
    "train_data['CAMEO_DEUG_2015'] = train_data['CAMEO_DEUG_2015'].replace(['X', 'XX'], np.nan)\n",
    "train_data['CAMEO_DEUG_2015']  = train_data['CAMEO_DEUG_2015'].astype(float)\n",
    "\n",
    "#creating a new feature indicating household wealth status from 'CAMEO HOUSEHOLD'\n",
    "train_data['CAMEO_HOUSEHOLD'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                              41, 42, 43, 44, 45, 51, 52, 53, 54, 55],[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5])\n",
    "#creating a new feature indicating family type from 'CAMEO HOUSEHOLD' \n",
    "train_data['CAMEO_FAMILY_TYPE'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "\n",
    "#dropping CAMEO_INTL_2015                                                                          \n",
    "train_data.drop(columns = ['CAMEO_INTL_2015'], inplace=True)                                                                             41, 42, 43, 44, 45, 51, 52, 53, 54, 55], [1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5])\n",
    "#OST_WEST_KZ encode 'W' to 0 and '0' to 1\n",
    "train_data['OST_WEST_KZ'] = train_data['OST_WEST_KZ'].replace(['W', 'O'], [0,1]) \n",
    "#one hot encoding of 'ANREDE_KZ' into 0 for male and 1 for female\n",
    "train_data = pd.get_dummies(train_data, columns=['ANREDE_KZ'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for WOHNLAGE, replace 0 with nan\n",
    "train_data['WOHNLAGE'] = train_data['WOHNLAGE'].replace([0], np.nan)\n",
    "#create new WOHNLAGE feature, urban or rural region\n",
    "train_data['WOHNLAGE_REGION'] = train_data['WOHNLAGE'].replace([1,2,3,4,5,7,8],[0,0,0,0,0,1,1])\n",
    "\n",
    "#dropping CAMEO_DEU, because they are nominal features and have too many categories (44 categoris) \n",
    "train_data.drop(columns = ['CAMEO_DEU_2015'], inplace=True)\n",
    "#dropping LP_LEBENSPHASE_FEIN, because they are nominal features and have too many categories (40 categoris)\n",
    "train_data.drop(columns = ['LP_LEBENSPHASE_FEIN'], inplace=True)\n",
    "#dropping D19_LETZTER_KAUF_BRANCHE, since it is duplicated information\n",
    "train_data.drop(columns=['D19_LETZTER_KAUF_BRANCHE'], inplace=True)\n",
    "\n",
    "\n",
    "#converting 'EINGEFUEGT_AM' to total months (1/1/2019 is used as the end date)\n",
    "eingefuegt = np.empty(shape=(train_data.shape[0],1))\n",
    "k = 0\n",
    "train_data['EINGEFUEGT_AM'] = train_data['EINGEFUEGT_AM'].fillna(-1)\n",
    "for i in train_data['EINGEFUEGT_AM']:\n",
    "    if i != -1:\n",
    "\n",
    "        yrmthday, zeros = i.split()\n",
    "        mth_1 = (2019 - (int(yrmthday[0:4]) + 1)) * 12\n",
    "        mth_2 = 12 - int(yrmthday[5:6])\n",
    "        mth_3 = (30 - int(yrmthday[8:10])) / 30\n",
    "        mth_total = mth_1 + mth_2 + mth_3\n",
    "        eingefuegt[k] = mth_total\n",
    "    else:\n",
    "        eingefuegt[k] = -1\n",
    "    k += 1\n",
    "    \n",
    "        \n",
    "#remove 'EINGEFUEGT_AM' and create a new feature 'EINGEFUEGT_MODIFIED'\n",
    "train_data.drop(columns =['EINGEFUEGT_AM'] , inplace=True)\n",
    "train_data['EINGEFUEGT_MODIFIED'] = pd.DataFrame(eingefuegt)\n",
    "#since i converted the NaN values to -1 earlier, i will need to replace the -1 value with mode value\n",
    "train_data['EINGEFUEGT_MODIFIED'] = train_data['EINGEFUEGT_MODIFIED'].replace([-1],train_data['EINGEFUEGT_MODIFIED'].mode().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ANZ_HAUSHALTE_AKTIV': 0, 'ANZ_PERSONEN': 0, 'BALLRAUM_2.0': 57, 'GEBAEUDETYP_8.0': 58, 'SEMIO_SOZ_3': 64, 'W_KEIT_KIND_HH_3.0': 72, 'INNENSTADT_7.0': 75, 'SEMIO_VERT_1': 77, 'GEBAEUDETYP_RASTER_5.0': 78, 'GFK_URLAUBERTYP_10.0': 78, 'SHOPPER_TYP_0': 78, 'MOBI_REGIO_3.0': 80, 'KKK_4.0': 81, 'SEMIO_FAM_3': 85, 'CJT_GESAMTTYP_1.0': 88, 'PLZ8_HHZ_5.0': 88, 'REGIOTYP_6.0': 90, 'CAMEO_DEUG_2015_1.0': 93, 'PLZ8_GBZ_5.0': 96, 'RETOURTYP_BK_S_3.0': 99, 'ONLINE_AFFINITAET_2.0': 104, 'RESPONSE': 395}\n"
     ]
    }
   ],
   "source": [
    "#this function fills NaN values with mean value for continuous features and mode value for nominal/ordinal features\n",
    "def impute_nan_values(df):  \n",
    "    continuous_features = ['EINGEZOGENAM_HH_JAHR' , 'MIN_GEBAEUDEJAHR', 'KBA13_ANZAHL_PKW', 'ANZ_HAUSHALTE_AKTIV','VERDICHTUNGSRAUM', 'ANZ_STATISTISCHE_HAUSHALTE']\n",
    "    \n",
    "    #fill continuous features nan with mean value\n",
    "    df[continuous_features] = df[continuous_features].fillna(np.mean(df[continuous_features]))\n",
    "\n",
    "    #list of categorical/ordinal features\n",
    "    all_features_except_continuous = df.columns.to_list()\n",
    "    for i in continuous_features:\n",
    "        all_features_except_continuous.remove(i)\n",
    "\n",
    "    #fill categorical features nan with most frequent values\n",
    "    for column in df[all_features_except_continuous].columns:\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#fill AZDIAS and CUSTOMER NaN values with mode/mean values\n",
    "train_data = impute_nan_values(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intially i used PCA to distill the features into principal components that can account for most of data variances and deal with correlated features. \n",
    "#However PCA can't handle redundant features that have little values in predicitng potential customers. \n",
    "#When I completed part 3 using XGBoost Classifier, I could extract the most important features in predicting potential customers.\n",
    "\n",
    "#I would like to find out if there is an optimal number of important features for predicting potential customers. \n",
    "#Therefore using the best hyperparameters combination, now I can retrain my model. First i used the trained model in the \n",
    "#supervised learning part to reaarange the features from most important to least important\n",
    "\n",
    "import pickle\n",
    "xgbcl = pickle.load(open(\"/home/workspace/xgbcl-best-model-for-mailout-train/xgbcl model for mailout train best model with loss 0.235243\", \"rb\"))\n",
    "sorted = np.argsort(xgbcl.feature_importances_)[::-1]\n",
    "\n",
    "impt_variables = []\n",
    "column_names = train_data.columns\n",
    "\n",
    "for i in sorted:\n",
    "    impt_variables.append(column_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next I would like to find out the optimal number of most important features to include in K-Means analysis\n",
    "#Therefore using the best hyperparameters combination, now I can retrain my model. I started training with 10 most important features and loop through additional ten features till I reached 270 features.\n",
    "# I will train my model on AZDIAS and CUSTOMER and predict on MAILOUT TRAIN\n",
    "\n",
    "#I will label AZDIAS as 0 and CUSTOMER dataset as 1\n",
    "response = np.empty(1082873)\n",
    "response[:891221]=0\n",
    "response[891221:]=1\n",
    "train_data = pd.concat([train_data.reset_index(drop=True),pd.DataFrame(response)], axis=1)\n",
    "\n",
    "\n",
    "#Next since I will be using MAILOUT_TRAIN as validation dataset, I would have to clean the MAILOUT_TRAIN dataset. Therefore i will perform the same preprocessing steps as before. I have aggregrated all the preprocessing\n",
    "#codes into a single function\n",
    "def cleaning_data(train_data):\n",
    "    columns_to_drop = ['ALTER_KIND1', 'ALTER_KIND2', 'ALTER_KIND3', 'ALTER_KIND4', 'EXTSEL992', 'KK_KUNDENTYP']\n",
    "    train_data.drop(columns = columns_to_drop, inplace=True)\n",
    "    k = 0\n",
    "    for i,j in var_unknown.items():\n",
    "        if i in columns_to_remove:\n",
    "            continue\n",
    "        if i in columns_to_rename:\n",
    "            train_data[columns_renamed[k]] = train_data[columns_renamed[k]].replace(j, np.nan)\n",
    "            continue\n",
    "        train_data[i] = train_data[i].replace(j, np.nan)\n",
    "    #'RELAT_AB' need to replace 9 with nan\n",
    "    train_data['RELAT_AB'] = train_data['RELAT_AB'].replace([9], np.nan)\n",
    "    columns_to_drop_1 = ['AGER_TYP', 'KBA05_BAUMAX', 'TITEL_KZ']\n",
    "    train_data.drop(columns = columns_to_drop_1, inplace=True)\n",
    "\n",
    "    #dropping 'LP_STATUS_GROB', 'LP_FAMILIE_GROB' because similar to 'LP_STATUS_FEIN' and 'LP_FAMILIE_FEIN' \n",
    "    train_data.drop(columns = ['LP_STATUS_GROB', 'LP_FAMILIE_GROB'], inplace=True)\n",
    "    #dropping LNR\n",
    "    train_data.drop(columns = ['LNR'], inplace=True)\n",
    "\n",
    "    #reengineering features\n",
    "    #PLZ8_BAUMAX, how many family homes\n",
    "    train_data['PLZ8_BAUMAX'] = train_data['PLZ8_BAUMAX'].replace([5], 0)\n",
    "    #if person lives in business building or not\n",
    "    train_data['PLZ8_BAUMAX_BIZ'] = train_data['PLZ8_BAUMAX'].replace([0,1,2,3,4], [1,0,0,0,0])\n",
    "    #main of avant-garde\n",
    "    train_data['PRAEGENDE_MAIN_OR_AVANT'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [0,1,0,1,0,1,1,0,1,0,1,0,1,0,1])\n",
    "    #40ies,50ies,60ies,70ies,80ies,90ies movements\n",
    "    train_data['PRAEGENDE_JUGENDJAHRE'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [1,1,2,2,3,3,3,4,4,5,5,5,5,6,6])\n",
    "\n",
    "    #CAMEO_INTL\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].astype(float)\n",
    "    train_data['CAMEO_DEUG_2015'] = train_data['CAMEO_DEUG_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_DEUG_2015']  = train_data['CAMEO_DEUG_2015'].astype(float)\n",
    "    #household income\n",
    "    train_data['CAMEO_HOUSEHOLD'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                  41, 42, 43, 44, 45, 51, 52, 53, 54, 55],[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5])\n",
    "    #family type \n",
    "    train_data['CAMEO_FAMILY_TYPE'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                   41, 42, 43, 44, 45, 51, 52, 53, 54, 55], [1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5])\n",
    "\n",
    "    #OST_WEST_KZ encode 'W' to 0 and '0' to 1\n",
    "    train_data['OST_WEST_KZ'] = train_data['OST_WEST_KZ'].replace(['W', 'O'], [0,1]) \n",
    "    #one hot encoding of 'ANREDE_KZ'\n",
    "    train_data = pd.get_dummies(train_data, columns=['ANREDE_KZ'], drop_first=True)\n",
    "\n",
    "\n",
    "    train_data.drop(columns = ['CAMEO_INTL_2015'], inplace=True)\n",
    "    #for WOHNLAGE, replace 0 with nan\n",
    "    train_data['WOHNLAGE'] = train_data['WOHNLAGE'].replace([0], np.nan)\n",
    "    #create new WOHNLAGE feature, urban or rural\n",
    "    train_data['WOHNLAGE_REGION'] = train_data['WOHNLAGE'].replace([1,2,3,4,5,7,8],[0,0,0,0,0,1,1])\n",
    "\n",
    "    #dropping CAMEO_DEU, too many categories \n",
    "    train_data.drop(columns = ['CAMEO_DEU_2015'], inplace=True)\n",
    "    #dropping LP_LEBENSPHASE_FEIN, too many categories\n",
    "    train_data.drop(columns = ['LP_LEBENSPHASE_FEIN'], inplace=True)\n",
    "    #dropping D19_LETZTER_KAUF_BRANCHE, duplicated information\n",
    "    train_data.drop(columns=['D19_LETZTER_KAUF_BRANCHE'], inplace=True)\n",
    "\n",
    "    #converting 'EINGEFUEGT_AM' to total months\n",
    "    eingefuegt = np.empty(shape=(train_data.shape[0],1))\n",
    "    k = 0\n",
    "    train_data['EINGEFUEGT_AM'] = train_data['EINGEFUEGT_AM'].fillna(-1)\n",
    "    for i in train_data['EINGEFUEGT_AM']:\n",
    "        if i != -1:\n",
    "\n",
    "            yrmthday, zeros = i.split()\n",
    "            mth_1 = (2019 - (int(yrmthday[0:4]) + 1)) * 12\n",
    "            mth_2 = 12 - int(yrmthday[5:6])\n",
    "            mth_3 = (30 - int(yrmthday[8:10])) / 30\n",
    "            mth_total = mth_1 + mth_2 + mth_3\n",
    "            eingefuegt[k] = mth_total\n",
    "        else:\n",
    "            eingefuegt[k] = -1\n",
    "        k += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data.drop(columns =['EINGEFUEGT_AM'] , inplace=True)\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = pd.DataFrame(eingefuegt)\n",
    "\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = train_data['EINGEFUEGT_MODIFIED'].replace([-1],train_data['EINGEFUEGT_MODIFIED'].mode().values)\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "#reading in MAILOUT TRAIN\n",
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';', index_col=0)\n",
    "#perform data cleaning and preprocessing\n",
    "mailout_train = cleaning_data(mailout_train)\n",
    "#since the 'RESPONSE' column is not at the end column, i recreated the \"RESPONSE\" column and rename it 'TARGET'\n",
    "mailout_train['TARGET'] = mailout_train['RESPONSE']\n",
    "mailout_train.drop(columns=['RESPONSE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will create 2 new variables representing AZDIAS + CUSTOMER response varialbe and MAILOUT TRAIN response variable\n",
    "#I started training with 10 most important features and loop through additional ten features till I reached 270 features.\n",
    "\n",
    "roc_pred = {}\n",
    "\n",
    "for i in range(10, 280, 10):   \n",
    " \n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    # I found tnis hyperparameter combination to yield the best result in UDACITY + ARVATO: Identify Customer Segments Kaggle competition AUC: 0.88149\n",
    "    #Therefore it is used here. XGBoost algorithm is used here since it gives the best result in Kaggle\n",
    "    xgbcl1 = xgb.XGBClassifier(colsample_bytree = 0.776792351030805, gamma = 1.0386435777290857, learning_rate = 0.10327932030577727, max_depth = 17, min_child_weight = 5, n_estimators = 571, reg_alpha = 0.19625707362167116, scale_pos_weight = 48, subsample = 0.8894945842180019, tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "    \n",
    "    #the validation dataset\n",
    "    eval_set = [(mailout_train.iloc[:,:i], y_test)]\n",
    "    #fitting XGBClassifier to AZDIAS + CUSTOMER dataset\n",
    "    xgbcl1.fit(train_data.iloc[:,:i], y_train, early_stopping_rounds=20, eval_metric=\"auc\", eval_set=eval_set)\n",
    "    #predicting on MAILOUT TRAIN\n",
    "    predictions=(xgbcl1.predict_proba(mailout_train.iloc[:,:i]))[:,1]\n",
    "    #recording ROC_AUC score for prediction in dictionary \n",
    "    roc_pred[i]=roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training the models, following is the list of AUC scores for the predictions on MAILOUT TRAIN using the trained models\n",
    "AUC_score = [0.7817697747196178, 0.8202459291641785, 0.8445464799165011, 0.8429744745436535, 0.8476595684355834, 0.8552984216374073, 0.8518630641534309, 0.8541272533797373, \n",
    "0.8531446531128671, 0.8567158158771899, 0.856455413516114, 0.8566308019045966, 0.8566638505880538, 0.8554695128110165, 0.8583080447406519, 0.8610451934101102, \n",
    "0.8575297393849932, 0.861461403036226, 0.8597985359344625, 0.8598775027954048, 0.8578174977273492, 0.8595900767119307, 0.8565992373108118, 0.8549426831278054,\n",
    "0.7928243156796069, 0.7855457861599557, 0.7781992100212822]\n",
    "\n",
    "#plotting the AUC scores versus number of most important features used in model training.\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(13, 6))\n",
    "\n",
    "feature = [i for i in range (10,280,10)]\n",
    "\n",
    "plt.bar(feature, AUC_score)\n",
    "\n",
    "plt.ylabel('ROC_AUC')\n",
    "plt.xlabel('Number of most important features')\n",
    "plt.ylim([0.75, 0.863])\n",
    "plt.xticks(feature)\n",
    "plt.show()\n",
    "\n",
    "#plot shows that selecting the 60 most important features is optimal, therefore we will be using the 60 most important features in performing K-Means analysis\n",
    "# it seems that using only 30 mostcimportant features can produce a sufficiently robust predictive model.\n",
    "train_data = train_data[impt_variables][0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we separate the train data back into AZDIAS and CUSTOMER\n",
    "\n",
    "customer_pop = train_data.iloc[891221:,:]\n",
    "train_data = train_data.iloc[:891221,:]\n",
    "\n",
    "#finding number of rows with minimum number of missing values\n",
    "missing3 = []\n",
    "for i in range(0,30,1):\n",
    "    l = 0    \n",
    "    n = 0\n",
    "    \n",
    "    for j in (missing1):\n",
    "        if j >= i: \n",
    "            l += 1\n",
    "        \n",
    "    missing3[i] = l\n",
    "\n",
    "#plotting bar graph of number of row with minimum no of missing rows from 1 until 14\n",
    "fig = plt.figure(figsize=(13, 6))\n",
    "feature1 = [i for i in range (1,14,1)]\n",
    "quantity1 = [ 329826, 321895, 320564, 318123, 157664, 155907, 125949, 124394, 124203, 117147, 107018, 106747, 106575]\n",
    "plt.bar(feature1 ,quantity1)\n",
    "\n",
    "plt.ylabel('Number of rows with missing values')\n",
    "plt.xlabel('No of features with missing value')\n",
    "plt.xticks(feature1)\n",
    "plt.show()\n",
    "\n",
    "#I decided to remove rows with minimun no of missing row = 5 since i only retained the 60 most important features\n",
    "train_data1 = train_data.dropna(axis=0,thresh=train_data.shape[1]-5)\n",
    "customer_pop1 = customer_pop.dropna(axis=0,thresh=train_data.shape[1]-5)\n",
    "\n",
    "#However for completeness of analysis, I thought that maybe I should perform K-Means analysis on dataset with 30 and 45 most important features\n",
    "#To ensure similar rows with missing values as the dataset with 60 features are removed, i find the rows index that are to be removed\n",
    "\n",
    "index_to_remove = set(train_data.index.to_list()) - set(train_data1.index.to_list())\n",
    "index_to_remove_1 = set(customer_pop.index.to_list()) - set(customer_pop1.index.to_list())\n",
    "\n",
    "#for 30 most important features\n",
    "customer_pop_30 = customer_pop.iloc[:,:30].drop(list(index_to_remove_1))\n",
    "train_data_30 = train_data.iloc[:,:30].drop(list(index_to_remove))\n",
    "#for 45 most important features\n",
    "customer_pop_45 = customer_pop.iloc[:,:45].drop(list(index_to_remove_1))\n",
    "train_data_45 = train_data.iloc[:,:45].drop(list(index_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before that PCA analysis is still useful for identifying interesting features, so we will conduct PCA here\n",
    "def pca_fit(df):\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import normalize\n",
    "    #normalize data\n",
    "    df = normalize(df)\n",
    "    #fit PCA on data\n",
    "    pca = PCA().fit(df)\n",
    "    \n",
    "    #only select components that cumulatively exceed 0.9 explaiend variance ratio\n",
    "    k = 0\n",
    "    m = 1\n",
    "    for i in pca.explained_variance_ratio_:\n",
    "        k += i\n",
    "        if k > 0.9:\n",
    "            break\n",
    "        m += 1\n",
    "\n",
    "    no_comp = m\n",
    "    \n",
    "    #fit PCA again on data with the desired number of component\n",
    "    df = PCA(n_components=m).fit_transform(df)\n",
    "    return pca, df\n",
    "\n",
    "pca, train_data_transformed = pca_fit(train_data)\n",
    "\n",
    "#Next we will find interesting variables identified by PCA\n",
    "\n",
    "# the set should contain the variable's string name (as found in X.colummns)\n",
    "def find_interesting_variables(pca, length_pca_component, top_length, feature_names):\n",
    "    interesting_variables = set()\n",
    " \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import normalize\n",
    "    \n",
    "    #how many PCA component to analyse\n",
    "    component = pca.components_[0:length_pca_component]\n",
    "    \n",
    "    #how many interesting variables sorted by most interesting to least interesting to return\n",
    "    for i in range(len(component)):\n",
    "    \n",
    "        feature_with_magnitude = dict(zip(feature_names, component))\n",
    "        sorted_feature_with_magnitude = sorted(feature_with_magnitude.items(), key=lambda item: item[1])\n",
    "        j = 0\n",
    "        for k,l in  sorted_feature_with_magnitude.items():\n",
    "            if j == top_length:\n",
    "                break\n",
    "            interesting_variables.add(k)\n",
    "            j += 1\n",
    "                    \n",
    "    return interesting_variables\n",
    "\n",
    "interesting_features = find_interesting_variables(pca, length_pca_component = 5, top_length = 8, feature_names = train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will perform K-Means analysis on 30,45 and 60 most important features dataset. I choose not to use elbow method to determine optimal number of clusters\n",
    "#as the objective of customer segmentation differs from elbow merhod objective.\n",
    "#instead i will conduct trial and errors and conduct K-Means analysis from 4 clusters to 15 clusters. Optimal number of cluster is chosen based on when a particular cluster\n",
    "#contain a significant proportion of examples in CUSTOMER dataset.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "for i in range(4,16):\n",
    "    km = KMeans(n_clusters=i , n_init=i+3, verbose=3, max_iter=500)\n",
    "    predicted_cluster_azdias = km.fit(train_data)\n",
    "    predicted_cluster_customer = km.predict(customer_pop)\n",
    "    \n",
    "    segment_count = {}\n",
    "    for i in range(count):\n",
    "        segment_count[i] = np.count_nonzero(array==i)\n",
    "        \n",
    "    print segment_count\n",
    "#After analysing the printed segment count, I decided that the optimal number of clusters is 9\n",
    "\n",
    "#Now i will perform K-Means analysis for 9 clusters on 30,45 and 60 most important features dataset\n",
    "\n",
    "#function to fit K-Means algortihm on dataset and then predict the clusters that dataset belong to\n",
    "def kmeans_segment(df, df2, no_cluster):\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=no_cluster , n_init=no_cluster+3, verbose=3, max_iter=500)\n",
    "    km.fit(df)\n",
    "    predicted_cluster_azdias = km.predict(df)\n",
    "    predicted_cluster_customer = km.predict(df2)\n",
    "    \n",
    "    return predicted_cluster_azdias, predicted_cluster_customer\n",
    "\n",
    "\n",
    "predicted_cluster_azdias, predicted_cluster_customer = kmeans_segment(train_data, customer_pop, 9)\n",
    "predicted_cluster_azdias1, predicted_cluster_custome1 = kmeans_segment(train_dat_30, customer_pop_30, 9)\n",
    "predicted_cluster_azdias2, predicted_cluster_customer2 = kmeans_segment(train_data_45, customer_pop_45, 9)\n",
    "\n",
    "#function to count how many examples are contained in each cluster\n",
    "def segmentation_count(array, count):\n",
    "    segment_count = {}\n",
    "    for i in range(count):\n",
    "        segment_count[i] = np.count_nonzero(array==i)\n",
    "    return segment_count\n",
    "\n",
    "segment_count_azdias = segmentation_count(array=predicted_cluster_azdias, count=9)\n",
    "segment_count_customer = segmentation_count(array=predicted_cluster_customer, count=9)\n",
    "\n",
    "print(segment_count_azdias)\n",
    "print(segment_count_customer)\n",
    "\n",
    "segment_count_azdias1 = segmentation_count(array=predicted_cluster_azdias1, count=9)\n",
    "segment_count_customer1 = segmentation_count(array=predicted_cluster_customer1, count=9)\n",
    "\n",
    "print(segment_count_azdias1)\n",
    "print(segment_count_customer1)\n",
    "\n",
    "segment_count_azdias2 = segmentation_count(array=predicted_cluster_azdias2, count=9)\n",
    "segment_count_customer2 = segmentation_count(array=predicted_cluster_customer2, count=9)\n",
    "\n",
    "print(segment_count_azdias2)\n",
    "print(segment_count_customer2)\n",
    "\n",
    "#For 60-features dataset, cluster 0 has the most customers, for 45-features dataset, will be cluster 8 and for 30 features dataset will be cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as can be seen cluster 0 has the most customers, 54491 or 38.8% out of all customers. There are 70103 potential customers in AZDIAS in cluster 0, or 8.9% out of total in AZDIAS\n",
    "#Therefore the mail order company can choose to target these potential customers.\n",
    "azdias_segment = [70103, 33633, 97520, 171205, 100766, 80193, 155008, 21552, 54530]\n",
    "customer_segment = [54491, 14133, 8278, 3175, 14689, 15772, 3207, 4414, 22195]\n",
    "\n",
    "azdias_segment_prop = [0.08935896291952938, 0.042871346445552, 0.12430689220022689, 0.21823176250143403, 0.12844450676218278, 0.1022204943212961, 0.1975857541650202, 0.02747192515073103, 0.06950835553402761]\n",
    "customer_segment_prop = [0.3882397366658592, 0.10069538452769426, 0.05897943770751101, 0.022621371674480244, 0.1046567963862804, 0.11237299970075666, 0.022849366601593115, 0.03144905025863175, 0.1581358564771934]\n",
    "\n",
    "#Plot bar graph for number of examples in each cluster (total 9 clusters) for 60 most important features\n",
    "fig = plt.figure(figsize=(13, 6))\n",
    "X = [i for i in range(9)]\n",
    "Y = [70103, 33633, 97520, 171205, 100766, 80193, 155008, 21552, 54530]\n",
    "Z = [54491, 14133, 8278, 3175, 14689, 15772, 3207, 4414, 22195]\n",
    "_X = np.arange(len(X))\n",
    "plt.title('Number of examples in each cluster (total 9 clusters)')\n",
    "plt.ylabel('Number of people')\n",
    "plt.xlabel('Cluster No.')\n",
    "plt.bar(_X - 0.2, Y, 0.4)\n",
    "plt.bar(_X + 0.2, Z, 0.4)\n",
    "plt.xticks(_X, X) # set labels manually\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plot bar graph for proportion of examples in each cluster (total 9 clusters) for 60 most important features\n",
    "fig = plt.figure(figsize=(13, 6))\n",
    "X = [i for i in range(9)]\n",
    "Y = [70103/784510, 33633/784510, 97520/784510, 171205/784510, 100766/784510, 80193/784510, 155008/784510, 21552/784510, 54530/784510]\n",
    "Z = [54491/140354, 14133/140354, 8278/140354, 3175/140354, 14689/140354, 15772/140354, 3207/140354, 4414/140354, 22195/140354]\n",
    "_X = np.arange(len(X))\n",
    "plt.title('Proportion of examples in each cluster (total 9 clusters)')\n",
    "plt.ylabel('Proportion of people')\n",
    "plt.xlabel('Cluster No.')\n",
    "plt.bar(_X - 0.2, Y, 0.4)\n",
    "plt.bar(_X + 0.2, Z, 0.4)\n",
    "plt.xticks(_X, X) # set labels manually\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since i conducted K-Means analysis on 30,45 and 60 features dataset, I would like to find out the intersection of potential customers in AZDIAS\n",
    "\n",
    "#First i will isolate the potential customers in AZDIAS and also customer in CUSTOMER belonging to the most overrepresented cluster\n",
    "\n",
    "#for 30 features\n",
    "predicted_cluster_azdias = pd.DataFrame(predicted_cluster_azdias)\n",
    "predicted_cluster_customer = pd.DataFrame(predicted_cluster_customer)\n",
    "predicted_cluster_azdias_label2 = predicted_cluster_azdias[predicted_cluster_azdias[0]==2]\n",
    "predicted_cluster_customer_label2 = predicted_cluster_customer[predicted_cluster_customer[0]==2]\n",
    "\n",
    "#for 60 features\n",
    "predicted_cluster_azdias1 = pd.DataFrame(predicted_cluster_azdias1)\n",
    "predicted_cluster_customer1 = pd.DataFrame(predicted_cluster_customer1)\n",
    "predicted_cluster_azdias1_label0 = predicted_cluster_azdias1[predicted_cluster_azdias1[0]==0]\n",
    "predicted_cluster_customer1_label0 = predicted_cluster_customer1[predicted_cluster_customer1[0]==0]\n",
    "\n",
    "#for 45 features\n",
    "predicted_cluster_azdias2 = pd.DataFrame(predicted_cluster_azdias2)\n",
    "predicted_cluster_customer2 = pd.DataFrame(predicted_cluster_customer2)\n",
    "predicted_cluster_azdias2 = predicted_cluster_azdias2[predicted_cluster_azdias2[0]==8]\n",
    "predicted_cluster_customer2 = predicted_cluster_customer2[predicted_cluster_customer2[0]==8]\n",
    "\n",
    "#finding intersection of potential customers between 30 ,45 and 60 most important features dataset\n",
    "predicted_cluster_azdias_intersect = np.intersect1d((np.intersect1d((predicted_cluster_azdias.index), (predicted_cluster_azdias1.index))), (predicted_cluster_azdias2.index))\n",
    "print(len(predicted_cluster_azdias_intersect))\n",
    "print(len(predicted_cluster_azdias.index))\n",
    "print(len(predicted_cluster_azdias1.index))\n",
    "print(len(predicted_cluster_azdias2.index))\n",
    "#there are 33720 potential customers common to 30, 45 and 60 features dataset. So, company can choose to target these potential core customers more vigorously\n",
    "\n",
    "#finding intersection of potential customers between 45 and 60 most important features dataset.\n",
    "predicted_cluster_azdias_intersect_1 = np.intersect1d((predicted_cluster_azdias1.index), (predicted_cluster_azdias2.index))\n",
    "print(len(predicted_cluster_azdias_intersect_1))\n",
    "print(len(predicted_cluster_azdias1.index))\n",
    "print(len(predicted_cluster_azdias2.index))\n",
    "#there are 53432 potential customers common to 45 and 60 features dataset. So, for a more conservative approach, company can choose to target these potential core customers more vigorously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, I would like to find the feature means for potential customers and customers belonging to cluster 0. Cluster 3 and 6 which represent non-customers in AZDIAS will also be examined \n",
    "\n",
    "train_data = pd.concat([train_data, predicted_cluster_azdias1.set_index(train_data.index)],axis=1)\n",
    "customer_pop = pd.concat([customer_pop, predicted_cluster_customer1.set_index(customer_pop.index)],axis=1)\n",
    "\n",
    "#finding feature means for cluster 0\n",
    "azdias_label_0 = train_data[train_data[0]==0]\n",
    "customer_label_0 = customer_pop[customer_pop[0]==0]\n",
    "azdias_label_0_mean = np.mean(general_pop_label_0)\n",
    "customer_label_0_mean = np.mean(customer_pop_label_0)\n",
    "\n",
    "#finding feature means for cluster 3\n",
    "azdias_label_3 = train_data[train_data[0]==3]\n",
    "customer_label_3 = customer_pop[customer_pop[0]==3]\n",
    "azdias_label_3_mean = np.mean(general_pop_label_3)\n",
    "customer_label_3_mean = np.mean(customer_pop_label_3)\n",
    "\n",
    "#finding feature means for cluster 6\n",
    "azdias_label_6 = train_data[train_data[0]==6]\n",
    "customer_label_6 = customer_po2[customer_pop[0]==6]\n",
    "azdias_label_6_mean = np.mean(general_pop_label_6)\n",
    "customer_6_mean = np.mean(customer_pop_label_6)\n",
    "\n",
    "label_0_3_6_mean = pd.concat([pd.DataFrame(azdias_label_0_mean),pd.DataFrame(customer_label_0_mean),pd.DataFrame(azdias_label_3_mean),pd.DataFrame(customer_label_3_mean), pd.DataFrame(azdias_label_6_mean),pd.DataFrame(customer_label_6_mean)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since this section is written after i have trained the model in supervised learning part for which i scored top in the leaderboard for the Kaggle competition with AUC 0.88149\n",
    "# I am curious to find out if potential customers and non-customers predicted by my best model for the MAILOUT TRAIN dataset share common traits as those in the AZDIAS dataset\n",
    "\n",
    "#I have already trained the model using 60 most important features that I will use to predict on MAILOUT TRAIN\n",
    "xgbcl1 = pickle.load(open(pd.read_csv(\"/home/workspace/xgbcl model  for 60 variables identifying feature importance.pkl\", \"rb\"))\n",
    "\n",
    "#concatenate predictions on Mailout Train (probability and binary response) with the correct response (target variable)\n",
    "predictions_1 =  xgbcl1.predict_proba(mailout_train.iloc[:,:60])[:,1]\n",
    "predictions_1a = xgbcl1.predict(mailout_train.iloc[:,:60])\n",
    "predictions_2 = pd.concat([pd.DataFrame(predictions_1),pd.DataFrame(predictions_1a).rename(columns={0:'1'}),pd.DataFrame(y_test)], axis=1 )\n",
    "#then concatenate the above dataframe with MAILOUT TRAIN dataset of 60 most important features\n",
    "mailout_train_modified = pd.concat([mailout_train.iloc[:,:60],predictions_2], axis=1)\n",
    "\n",
    "#I have chosen a cutoff of >0.99 probability predicted on MAILOUT TRAIN to represent my pool of potential customers. 0.99 is chosen because in part 3, I found that a cutoff of 0.99\n",
    "#enables me to predict most of the people who responded positively to the mailout campaign while substantially reducing the number of people wrongly predicted to respond positively\n",
    "#therefore i will elaborate more on this in part 3\n",
    "\n",
    "# this amounts to 6586 people out of which 365 are correctly predicted to be customers. (365/532 = 68.6%)\n",
    "mailout_modified_99 = mailout_train_modified[mailout_train_modified[0]>0.99]\n",
    "mailout_modified_99_mean = np.mean(mailout_modified_99, axis=0)\n",
    "\n",
    "#cutoff of 0.04 is chosen on the basis that the number of non-customers predicted by my model will be similar to that of those predicted to be potential customers.\n",
    "#this amount to 6898 people, of which 7 is wrongly predicted to be customers. Therefore, the number of potential customers and non-customers are pretty balanced.\n",
    "mailout_modified_04 = mailout_train_modified[mailout_train_modified[0]<=0.04]\n",
    "mailout_modified_04_mean = np.mean(mailout_mod_04)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Now to supervised learning part firstly we need to perform the same data preprocessing and cleaning as earlier\n",
    "def cleaning_data(train_data):\n",
    "    columns_to_drop = ['ALTER_KIND1', 'ALTER_KIND2', 'ALTER_KIND3', 'ALTER_KIND4', 'EXTSEL992', 'KK_KUNDENTYP']\n",
    "    train_data.drop(columns = columns_to_drop, inplace=True)\n",
    "    k = 0\n",
    "    for i,j in var_unknown.items():\n",
    "        if i in columns_to_remove:\n",
    "            continue\n",
    "        if i in columns_to_rename:\n",
    "            train_data[columns_renamed[k]] = train_data[columns_renamed[k]].replace(j, np.nan)\n",
    "            continue\n",
    "        train_data[i] = train_data[i].replace(j, np.nan)\n",
    "    #'RELAT_AB' need to replace 9 with nan\n",
    "    train_data['RELAT_AB'] = train_data['RELAT_AB'].replace([9], np.nan)\n",
    "    columns_to_drop_1 = ['AGER_TYP', 'KBA05_BAUMAX', 'TITEL_KZ']\n",
    "    train_data.drop(columns = columns_to_drop_1, inplace=True)\n",
    "\n",
    "    #dropping 'LP_STATUS_GROB', 'LP_FAMILIE_GROB' because similar to 'LP_STATUS_FEIN' and 'LP_FAMILIE_FEIN' \n",
    "    train_data.drop(columns = ['LP_STATUS_GROB', 'LP_FAMILIE_GROB'], inplace=True)\n",
    "    #dropping LNR\n",
    "    train_data.drop(columns = ['LNR'], inplace=True)\n",
    "\n",
    "    #reengineering features\n",
    "    #PLZ8_BAUMAX, how many family homes\n",
    "    train_data['PLZ8_BAUMAX'] = train_data['PLZ8_BAUMAX'].replace([5], 0)\n",
    "    #if person lives in business building or not\n",
    "    train_data['PLZ8_BAUMAX_BIZ'] = train_data['PLZ8_BAUMAX'].replace([0,1,2,3,4], [1,0,0,0,0])\n",
    "    #main of avant-garde\n",
    "    train_data['PRAEGENDE_MAIN_OR_AVANT'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [0,1,0,1,0,1,1,0,1,0,1,0,1,0,1])\n",
    "    #40ies,50ies,60ies,70ies,80ies,90ies movements\n",
    "    train_data['PRAEGENDE_JUGENDJAHRE'] = train_data['PRAEGENDE_JUGENDJAHRE'].replace([i for i in range(1,16)], [1,1,2,2,3,3,3,4,4,5,5,5,5,6,6])\n",
    "\n",
    "    #CAMEO_INTL\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_INTL_2015'] = train_data['CAMEO_INTL_2015'].astype(float)\n",
    "    train_data['CAMEO_DEUG_2015'] = train_data['CAMEO_DEUG_2015'].replace(['X', 'XX'], np.nan)\n",
    "    train_data['CAMEO_DEUG_2015']  = train_data['CAMEO_DEUG_2015'].astype(float)\n",
    "    #household income\n",
    "    train_data['CAMEO_HOUSEHOLD'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                  41, 42, 43, 44, 45, 51, 52, 53, 54, 55],[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5])\n",
    "    #family type \n",
    "    train_data['CAMEO_FAMILY_TYPE'] = train_data['CAMEO_INTL_2015'].replace([11, 12, 13, 14, 15, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35,\n",
    "                                                                                   41, 42, 43, 44, 45, 51, 52, 53, 54, 55], [1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5])\n",
    "\n",
    "    #OST_WEST_KZ encode 'W' to 0 and '0' to 1\n",
    "    train_data['OST_WEST_KZ'] = train_data['OST_WEST_KZ'].replace(['W', 'O'], [0,1]) \n",
    "    #one hot encoding of 'ANREDE_KZ'\n",
    "    train_data = pd.get_dummies(train_data, columns=['ANREDE_KZ'], drop_first=True)\n",
    "\n",
    "\n",
    "    train_data.drop(columns = ['CAMEO_INTL_2015'], inplace=True)\n",
    "    #for WOHNLAGE, replace 0 with nan\n",
    "    train_data['WOHNLAGE'] = train_data['WOHNLAGE'].replace([0], np.nan)\n",
    "    #create new WOHNLAGE feature, urban or rural\n",
    "    train_data['WOHNLAGE_REGION'] = train_data['WOHNLAGE'].replace([1,2,3,4,5,7,8],[0,0,0,0,0,1,1])\n",
    "\n",
    "    #dropping CAMEO_DEU, too many categories \n",
    "    train_data.drop(columns = ['CAMEO_DEU_2015'], inplace=True)\n",
    "    #dropping LP_LEBENSPHASE_FEIN, too many categories\n",
    "    train_data.drop(columns = ['LP_LEBENSPHASE_FEIN'], inplace=True)\n",
    "    #dropping D19_LETZTER_KAUF_BRANCHE, duplicated information\n",
    "    train_data.drop(columns=['D19_LETZTER_KAUF_BRANCHE'], inplace=True)\n",
    "\n",
    "    #converting 'EINGEFUEGT_AM' to total months\n",
    "    eingefuegt = np.empty(shape=(train_data.shape[0],1))\n",
    "    k = 0\n",
    "    train_data['EINGEFUEGT_AM'] = train_data['EINGEFUEGT_AM'].fillna(-1)\n",
    "    for i in train_data['EINGEFUEGT_AM']:\n",
    "        if i != -1:\n",
    "\n",
    "            yrmthday, zeros = i.split()\n",
    "            mth_1 = (2019 - (int(yrmthday[0:4]) + 1)) * 12\n",
    "            mth_2 = 12 - int(yrmthday[5:6])\n",
    "            mth_3 = (30 - int(yrmthday[8:10])) / 30\n",
    "            mth_total = mth_1 + mth_2 + mth_3\n",
    "            eingefuegt[k] = mth_total\n",
    "        else:\n",
    "            eingefuegt[k] = -1\n",
    "        k += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data.drop(columns =['EINGEFUEGT_AM'] , inplace=True)\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = pd.DataFrame(eingefuegt)\n",
    "\n",
    "    train_data['EINGEFUEGT_MODIFIED'] = train_data['EINGEFUEGT_MODIFIED'].replace([-1],train_data['EINGEFUEGT_MODIFIED'].mode().values)\n",
    "    \n",
    "    return train_data\n",
    "    \n",
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';', index_col=0)\n",
    "mailout_train = cleaning_data(mailout_train)\n",
    "mailout_train['TARGET'] = mailout_train['RESPONSE']\n",
    "mailout_train.drop(columns=['RESPONSE'], inplace=True)\n",
    "\n",
    "#I would need to read in MAILOUT TEST and perform the same cleaning and preprocessing steps as before\n",
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';', index_col=0)\n",
    "mailout_test = cleaning_data(mailout_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting MAILOUT TRAIN, there is only 532 positive responses, hence this is an imbalanced dataset\n",
    "print(mailout_train.shape[0])\n",
    "print(np.count_nonzero(mailout_train['TARGET'] == 1))\n",
    "\n",
    "#firstly we will train a model using MAILOUT TRAIN as the training data and 'TARGET' variable as the target variable\n",
    "#I will only mention XGBoost here since it gives me the highest AUC score in Kaggle competition\n",
    "\n",
    "# I will be using bayesian optimisation to find the hyperparameter combination that give the best cross-validation mean score which is then uploaded to Kaggle.\n",
    "import ast\n",
    "import csv\n",
    "import inspect\n",
    "import sys\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import tensorflow_addons as tfa\n",
    "predictions_all = pd.DataFrame()\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score)\n",
    "\n",
    "#used to name the csv file for recording cross validation results and hyperparameters combination\n",
    "results_file = \"bayes_optimization_summary_{}.csv\".format(\n",
    "    strftime(\"%y%m%d%H%M\", gmtime())\n",
    ")\n",
    "\n",
    "# add file header, we will record the cross validation results along with the hyperparameters in a csv file\n",
    "with open(results_file, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"iteration\",\n",
    "            \"loss\",\n",
    "            \"overall_score\",\n",
    "            \"mean_score\",\n",
    "            \"std_score\",\n",
    "            \"min_score\",\n",
    "            \"max_score\",\n",
    "            \"train_time\",\n",
    "            \"params\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    global prediction_all\n",
    "\n",
    "    xgbcl = xgb.XGBClassifier(colsample_bytree =  space['colsample_bytree'], gamma = space['gamma'], learning_rate = space['learning_rate'], max_depth = space['max_depth'], min_child_weight = space['min_child_weight'], n_estimators = space['n_estimators'], reg_alpha = space['reg_alpha'], scale_pos_weight = space['scale_pos_weight'], subsample = space['subsample'], tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "\n",
    "    # execute cross-validation scoring, I will used stratified cross-validation and 3 cross-validation folds\n",
    "    # scoring will be roc_auc since this is an imbalanced dataset\n",
    "    cv = cross_val_score(\n",
    "        estimator=xgbcl,\n",
    "        X=mailout_train.iloc[:,:-1],\n",
    "        y=mailout_train.iloc[:,-1],\n",
    "        verbose=True,\n",
    "        n_jobs=-1,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=False, random_state=None),\n",
    "        scoring='roc_auc',\n",
    "    )\n",
    "    #this fit the XGBClassifier to the MAILOUT TRAIN and fitted model is used to predict on MAILOUT TEST and predictions are concatenated to predictions_all variable\n",
    "    xgbcl.fit(mailout_train.iloc[:,:-1], mailout_train.iloc[:,-1])\n",
    "    predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "    predictions_all = pd.concat([predictions_all, predictions_xgbcl], axis=1)\n",
    "    \n",
    "\n",
    "    # log runtime\n",
    "    run_time = timer() - start\n",
    "    # calculate loss, mean cross validation score, standard deviation and minimum and maximum cross-validation score\n",
    "   \n",
    "    loss = 1 - cv.mean()\n",
    "    mean_score = cv.mean()\n",
    "    std_score = cv.std()\n",
    "    min_score = cv.min()\n",
    "    max_score = cv.max()\n",
    "\n",
    "    # export results to csv\n",
    "    out_file = results_file\n",
    "    with open(out_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                ITERATION,\n",
    "                \n",
    "                loss,\n",
    "                mean_score,\n",
    "                std_score,\n",
    "                min_score,\n",
    "                max_score,\n",
    "                run_time,\n",
    "                space,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return loss\n",
    "\n",
    "#this is the hyperparameter search space for XGBoost Classifier\n",
    "space = { 'learning_rate': hp.uniform('learning_rate', 0.001, 0.050),\n",
    "'max_depth': hp.choice('max_depth', np.arange(3, 10, 1, dtype=int)),\n",
    "'min_child_weight': hp.choice('min_child_weight', np.arange(1, 20, 1, dtype = int)),\n",
    "'gamma': hp.uniform('gamma', 0.04, 0.4),\n",
    "'subsample': hp.uniform('subsample', 0.5, 0.8),\n",
    "'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "'reg_alpha': hp.uniform('reg_alpha', 0.002 , 0.04),\n",
    "'n_estimators': hp.choice('n_estimators', np.arange(400, 600,1, dtype = int)),\n",
    "'scale_pos_weight': hp.choice('scale_pos_weight', np.arange(10, 50, 1, dtype=int)),\n",
    "'tree_method' :'gpu_hist',\n",
    "'sampling_method' : 'gradient_based'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line of codes initiates the bayesian optimisation procedure and I will set maximum iterations at 300\n",
    "ITERATION=0\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=300,\n",
    "    trials=Trials(),\n",
    "    show_progressbar=True,\n",
    ")\n",
    "#this saves all the predictions on MAILOUT TEST to a csv file\n",
    "pd.DataFrame(predictions_all).to_csv('xgbcl model predictions probability on MAILOUT TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following is the hyperparameter combination that provides the best result in Kaggle\n",
    "hyperparam = {'colsample_bytree': 0.5027067891629291, 'gamma': 0.0597643487533388, 'learning_rate': 0.001370272785081644, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 505, 'reg_alpha': 0.005424468447439956, 'sampling_method': 'gradient_based', 'scale_pos_weight': 40, 'subsample': 0.46899673182868984, 'tree_method': 'gpu_hist'}\n",
    "\n",
    "xgbcl4 = xgb.XGBClassifier(colsample_bytree =  0.5027067891629291, gamma = 0.0597643487533388, learning_rate = 0.001370272785081644, max_depth = 4, min_child_weight = 1, n_estimators = 505, reg_alpha = 0.005424468447439956, scale_pos_weight = 40, subsample = 0.46899673182868984, tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "xgbcl4.fit(mailout_train.iloc[:,:-1], mailout_train.iloc[:,-1])\n",
    "\n",
    "import pickle\n",
    "pickle.dump(xgbcl4, open(\"best xgbcl model for training on MAILOUT TRAIN.pkl\" ,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have tried various methods of raising my AUC score in the Kaggle competition and I found that choosing the 130 most important features as predicted by the above best xgbcl\n",
    "#model as features and using AZDIAS + CUSTOMER as the training dataset and MAILOUT TRAIN as the validation dataset, I can achieve the highest AUC score in Kaggle competition\n",
    "\n",
    "#Using the K-Means analysis above for 60 most important features, i will label cluster 0 in AZDIAS as 1 (positive response) and the rest as 0. \n",
    "#while examples in CUSTOMER will all be labelled 0. I choose not to drop any row because I find that not dropping any row raises my AUC score on validation dataset\n",
    "\n",
    "#Therefore we read the raw AZDIAS and CUSTOMER dataset again and perform the same preprocessing and cleaning steps as before\n",
    "train_data = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';', index_col=0)\n",
    "customer_data = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';', index_col=0)\n",
    "\n",
    "#invoking the function that cleans and preprocesses AZDIAS and CUSTOMER dataset\n",
    "# we will not fill missing values with mode/mean since XGBoost can handle missing values\n",
    "train_data = cleaning_data(train_data)\n",
    "customer_data = cleaning_data(customer_data)\n",
    "\n",
    "#we use our best XGBoost model earlier to determine the 130 most important features which will be used as features in the combined AZDIAS and CUSTOMER dataset.\n",
    "\n",
    "#this sorts features from most important to least important\n",
    "sorted = np.argsort(xgbcl4.feature_importances_)[::-1]\n",
    "\n",
    "#this appends the feature names starting from most important to least important to impt_variables list \n",
    "impt_variables = []\n",
    "column_names = train_data.columns \n",
    "for i in sorted:\n",
    "    impt_variables.append(column_names[i])\n",
    "impt_130 = impt_variables[:130]\n",
    "\n",
    "#this converts the ASDIAZ and CUSTOMER data earlier to only include the 130 most important features\n",
    "train_data = train_data[impt_130]\n",
    "customer_data = customer_data[impt_130]\n",
    "\n",
    "#since our earlier K-Means analysis identified cluster 0 as potential customers in AZDIAS, we will label cluster 0 as 1 (positive response)\n",
    "#and the other clusters as 0 (negative response)\n",
    "predicted_label_azdias = pd.DataFrame(predicted_cluster_azdias1).replace([i for i in range(1,10)] ,[0 for i in range(0,9)])\n",
    "predicted_label_azdias = (predicted_label_azdias).replace([0],1)\n",
    "#for CUSTOMER dataset, we will label all examples as 0\n",
    "predicted_label_customer = np.array(customer_data.shape[0])\n",
    "\n",
    "# this concatenate both the AZDIAS and CUSTOMER label\n",
    "predicted_label = pd.concat([pd.DataFrame(predicted_label_azdias), pd.DataFrame(predicted_label_customer)], axis=0)\n",
    "#this concatenate both AZDIAS and CUSTOMER dataset along with the label\n",
    "train_data = pd.concat([train_data, customer_data], axis=0)\n",
    "train_data = pd.concat([train_data.reset_index(drop=True), predicted_label], axis=1)\n",
    "#now we convert the MAILOUT TRAIN to only include the 130 most important features\n",
    "mailout_train = mailout_train[impt_130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Bayesian Optimisation again. For the hyperparameter max_depth, after conducting Bayesian Optimisation for many hundreds iterations, I found that max_depth 16 yields the best AUC score in validation dataset\n",
    "#as such max_depth range will only be set to 16 and as for other hyperparameters, I progressively narrowed my search range by looking up trained models with hyperparameter combinations that yield the best AUC score in both mailout train and mailout test. \n",
    "#As such, I was able to arrive at a hyperparameters combination with max_depth of 16 that produce the highest AUC score in the leaderboard, 0.88149. \n",
    "#I have tried about 30 more times to best my result. However, my effort was futile and I persuaded myself to take satisfaction in my current result. \n",
    "\n",
    "\n",
    "import ast\n",
    "import csv\n",
    "import inspect\n",
    "import sys\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "predictions_all_1 = pd.DataFrame()\n",
    "validation_data = mailout_train.copy()\n",
    "\n",
    "#I will split validation data(MAILOUT TRAIN) into three parts because in the Kaggle competition, it states only 30% of MAILOUT TEST will be used for evaluation\n",
    "# I also have to ensure each part has equal number of RESPONSE of 1 since this is an imbalanced dataset.\n",
    "mailout_train_zeros = validation_data[validation_data['TARGET']==0]\n",
    "mailout_train_ones = validation_data[validation_data['TARGET']==1]\n",
    "mailout_train_sample1 = np.array(pd.concat([mailout_train_zeros.iloc[:13000,:],mailout_train_ones.iloc[:177,:]]).sample(frac=1, random_state =42))\n",
    "mailout_train_sample2 = np.array(pd.concat([mailout_train_zeros.iloc[13000:26000,:],mailout_train_ones.iloc[177:354,:]]).sample(frac=1, random_state =42))\n",
    "mailout_train_sample3 = np.array(pd.concat([mailout_train_zeros.iloc[26000:42962,:],mailout_train_ones.iloc[354:532,:]]).sample(frac=1, random_state =42))\n",
    "validation_data = np.array(validation_data)\n",
    "\n",
    "#used to name the csv file for recording cross validation results and hyperparameters combination\n",
    "results_file = \"bayes_optimization_summary_{}.csv\".format(\n",
    "    strftime(\"%y%m%d%H%M\", gmtime())\n",
    ")\n",
    "\n",
    "# add file header, we will record the cross validation results along with the hyperparameters in a csv file\n",
    "with open(results_file, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"iteration\",\n",
    "            \"loss\",\n",
    "            \"overall_score\",\n",
    "            \"mean_score\",\n",
    "            \"std_score\",\n",
    "            \"min_score\",\n",
    "            \"max_score\",\n",
    "            \"train_time\",\n",
    "            \"params\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "def objective(space):\n",
    "\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    global prediction_all\n",
    "\n",
    "\n",
    "    # used to store the auc scores for all three parts of MAILOUT TRAIN.\n",
    "    auc_score = np.empty(3)\n",
    "    \n",
    "    #training the XGBClassifier on the combined AZDIAS and CUSTOMER dataset with MAILOUT TRAIN as the validation dataset and early stopping rounds set to 40 \n",
    "    eval_set = [(validation_data[:, :-1], validation_data[:, -1])]\n",
    "    xgbcl = xgb.XGBClassifier(colsample_bytree =  space['colsample_bytree'], gamma = space['gamma'], learning_rate = space['learning_rate'], max_depth = space['max_depth'], min_child_weight = space['min_child_weight'], n_estimators = space['n_estimators'], reg_alpha = space['reg_alpha'], scale_pos_weight = space['scale_pos_weight'], subsample = space['subsample'], tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "    xgbcl.fit(np.array(train_data.iloc[:,:-1]), np.array(train_data.iloc[:,-1]), early_stopping_rounds=40, eval_metric=\"auc\", eval_set=eval_set)\n",
    "    \n",
    "    #trained XGBClassifier model is then used to predict on overall MAILOUT TRAIN data and also the 3 separate parts\n",
    "    prediction = (xgbcl.predict_proba(validation_data[:,:-1]))[:,1]\n",
    "    score_a = roc_auc_score(validation_data[:,-1], prediction)\n",
    "    predictions1 = (xgbcl.predict_proba(mailout_train_sample1[:, :-1]))[:, 1]\n",
    "    auc_score[0]=roc_auc_score(mailout_train_sample1[:, -1], predictions1)\n",
    "    predictions2 = (xgbcl.predict_proba(mailout_train_sample2[:, :-1]))[:, 1]\n",
    "    auc_score[1]=roc_auc_score(mailout_train_sample2[:, -1], predictions2)\n",
    "    predictions3 = (xgbcl.predict_proba(mailout_train_sample3[:, :-1]))[:, 1]\n",
    "    auc_score[2]=roc_auc_score(mailout_train_sample3[:, -1], predictions3)\n",
    "    \n",
    "     \n",
    "    #then trained XGBClassifier model is used to predict on MAILOUT TEST and predictions are concatenated to predictions_all_1 variable  \n",
    "    predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "    predictions_all_1 = pd.concat([predictions_all, predictions_xgbcl], axis=1)\n",
    "    \n",
    "   \n",
    "    # log runtime\n",
    "    run_time = timer() - start\n",
    "\n",
    "    # calculate mean loss, mean, min and max auc scores over the 3 parts and overall auc scores on the whole MAILOUT TRAIN dataset\n",
    "\n",
    "    loss = 1 - auc_score.mean()\n",
    "    mean_score = auc_score.mean()\n",
    "    std_score = auc_score.std()\n",
    "    min_score = auc_score.min()\n",
    "    max_score = auc_score.max()\n",
    "    overall_score = score_a\n",
    "\n",
    "    # export results to csv\n",
    "    out_file = results_file\n",
    "    with open(out_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                ITERATION,\n",
    "                loss,\n",
    "                overall_score,\n",
    "                mean_score,\n",
    "                std_score,\n",
    "                min_score,\n",
    "                max_score,\n",
    "                run_time,\n",
    "                space\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I progressively narrowed my search range by looking up trained models with hyperparameter combinations that yield the best AUC score in both mailout train and mailout test. \n",
    "#Below is the refined search space for hyperparameters combination that yield the best AUC score on validation dataset.\n",
    "#As such, I was able to arrive at a hyperparameters combination with max_depth of 16 that produce the highest AUC score in the leaderboard, 0.88149.\n",
    "\n",

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line of codes initiates the bayesian optimisation procedure and I will set maximum iterations at 200\n",
    "ITERATION=0\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=200,\n",
    "    trials=Trials(),\n",
    "    show_progressbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer  this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following is the hyperparameters combination that provides the best result in Kaggle, namely an AUC of 0.88149\n",
    "hyperparam_best = {'colsample_bytree': 0.776792351030805, 'gamma': 1.0386435777290857, 'learning_rate': 0.10327932030577727, 'max_depth': 16, 'min_child_weight': 5, 'n_estimators': 571, 'reg_alpha': 0.19625707362167116, 'sampling_method': 'gradient_based', 'scale_pos_weight': 48, 'subsample': 0.8894945842180019, 'tree_method': 'gpu_hist'}\n",
    "\n",
    "#training XGBClassifier using the best hyperparameters combination on the combined AZDIAS and CUSTOMER dataset\n",
    "xgbcl5 = xgb.XGBClassifier(colsample_bytree = 0.776792351030805, gamma = 1.0386435777290857, learning_rate = 0.10327932030577727, max_depth = 17, min_child_weight = 5, n_estimators = 571, reg_alpha = 0.19625707362167116, scale_pos_weight = 48, subsample = 0.8894945842180019, tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "# as before i used MAILOUT TRAIN as my validation dataset with early stopping rounds set at 40.\n",
    "eval_set = [(mailout_train[:, :-1], mailout_train[:, -1])]\n",
    "xgbcl5.fit(np.array(train_data.iloc[:,:-1]), np.array(train_data.iloc[:,-1]), early_stopping_rounds=40, eval_metric=\"auc\", eval_set=eval_set)\n",
    "#find the AUC for predictions on MAILOUT TRAIN\n",
    "prediction = (xgbcl5.predict_proba(mailout_train[:,:-1]))[:,1]\n",
    "score_auc = roc_auc_score(mailout_train[:,-1], prediction)\n",
    "\n",
    "#use the trained XGBClassifier model to predict on MAILOUT TEST and which will be submitted to Kaggle competition\n",
    "#convert MAILOUT TEST to only include the 130 most important features\n",
    "mailout_test = mailout_test[impt_130]\n",
    "\n",
    "#would need to obtain the LNR column for submission to Kaggle\n",
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', index_col=0)\n",
    "mailout_test_lnr = mailout_test['LNR']\n",
    "\n",
    "#predict on the MAILOUT TEST\n",
    "predictions_xgbcl = (xgbcl.predict_proba(mailout_test))[:, 1]\n",
    "#concatenate predictions with LNR for submission to Kaggle\n",
    "predictions_xgbcl = pd.concat([mailout_test_lnr, predictions_xgbcl], axis = 1, ignore_index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
